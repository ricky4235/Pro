{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC5-4%E8%AC%9B-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E9%80%B2%E9%9A%8E%E5%AF%A6%E7%94%A8%E6%8A%80%E5%B7%A7-%E6%AD%A3%E8%A6%8F%E5%8C%96-8dd14fcd3140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://miro.medium.com/max/300/1*B_qXfG6_LTy_qNVXDxokxw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting的狀況\n",
    "* Model在Training的Data正確率很高，但是拿到Testing Data的時候錯誤率卻很高。\n",
    "* 背後的主因是我們真正需要的Model比我們Train出來的Model還要簡單，也就是Train出來的Model太複雜了！\n",
    "* 如上圖所示假設我們需要的Model是一條回歸的直線，但是Train出來的Model為了讓在Traing Data中錯誤率最小化，因此Model變得奇形怪狀，這樣的Model拿去新的資料中錯誤率就會很高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這時候通常有幾種解決方法：\n",
    "1. 收集更多的Training Data\n",
    "2. 減少資料的維度(特徵)\n",
    "3. 使用更簡單的Model\n",
    "4. 對現有的Model加上使用L1 or L2正規化(懲罰penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面有提到說我們Train出來的Model其實就是在空間中的一個多項式，<br>\n",
    "像是： W0*X0 + W1*X1 +…..Wn*Xn  <br>\n",
    "複雜一點的Model可能是： W0*X0 + W1*X1²+W2*X2³ + … + Wn*Xn¹⁰ ，<br>\n",
    "如果我們要降低這些多項式Model的複雜程度，最常見的方式就是限制W的範圍，讓W越小越好，甚至變成0。<br>\n",
    "而限制W的方式主要有兩種<br>\n",
    "\n",
    "1. L1正規化\n",
    "2. L2正規化\n",
    "在Train Model時都是要找一組W讓整體的錯誤率最小，通常稱作最小cost，這個值就如圓心的點。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://miro.medium.com/max/700/1*bzuZfFSrisZtyD6ucngbTg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1正規化就是在最小Cost的公式加上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://miro.medium.com/max/254/1*iE-oUe6z6UsSFke3ZH8i9A.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "整體最小就變成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://miro.medium.com/max/700/1*b9Y0iFdzgV6Dah5Zu4cwRQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2正規化則是在最小Cost的公式加上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://miro.medium.com/max/246/1*VUYY6iQm4kMrWAAczbJIRg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "整體最小就變成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://miro.medium.com/max/684/1*Dtc18UeVkxJCe8LFAOiRhg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這兩者的差別就很數學了，L1比較容易造成稀疏解(也就是很多的Wi會等於0)，以上面的例子來說在整體最小的情況下w2 =1, w1=0，這邊不深入討論兩者細微的差異了。原則上比較常用的正規化是L2，像是Scikit-Learn 內建的Penalty就是使用L2正規化。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
