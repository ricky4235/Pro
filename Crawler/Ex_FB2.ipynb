{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ex_FB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python網路爬蟲應用-facebook社團成員參與度分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "前言\n",
    "我一位朋友在經營facebook社團，需要每個月統計成員的參與度，例如某某人PO幾篇文章、按了幾次讚、留言幾次，諸如此類。向我尋求協助，於是就編寫了一段簡單的臉書爬蟲程式，自動統計這些數值。\n",
    "\n",
    "原本facebook有提供後台API，可以讓人非常輕鬆的撈取資料，然而facebook在幾波個資外洩風暴的影響下，如2018年3月爆發的劍橋事件等，使得此API的功能被設下重重限制，難以像以前一樣輕鬆撈取資料。現今只能使用上篇教學介紹的selenium進行爬蟲。\n",
    "\n",
    "本篇將會介紹如何使用selenium登入facebook，前往目標社團，解析原始碼，抓取目標資訊，學會本篇的內容後facebook就變成隨你爬的遊樂場了。\n",
    "\n",
    "本篇教學會以台灣資料工程協會 為範例進行，你們可以改成任意臉書社團。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "目的\n",
    "使用python編寫自動化程式，抓取臉書OOO社團內一個月內，各社團成員的活動紀錄，包含: 張貼文章次數、留言次數、回覆表情符號次數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "制定爬取策略\n",
    "將社團中的貼文、留言、表情符號回覆的資料中誰與何時抓取下來，如果時間在這一個月內，則把這筆紀錄留下，最後在統計每位的參與度。\n",
    "\n",
    "誰在何時貼文或留言的資訊在臉書上都一目瞭然，因此只需要直接剖析原始碼，即可得知。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "其中有許多資訊需要使用滑鼠點開才會顯示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "表情符號的部分，則須再前往另外一個分頁，才能看到那些人在那個時間對這篇文章或留言回覆。\n",
    "\n",
    "在表情符號處點下後，會開啟另外的一個小視窗，一一顯示那些人回覆什麼表情符號。也可以將這裡的超連結保存起來，直接以瀏覽器前往。\n",
    "\n",
    "以這裡為例，共有三位成員回覆表情符號，但是並沒有顯示何時按下表情符號。按讚的時間一定會在原始貼文或留言的時間之後，因此還是能判斷這個讚是在這一個月內按下去的。但如果是對一篇一個月以前的文章按讚的話，就無法分別，只能無視。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "觀察到這裡，應該已經能夠擬出一套爬取策略了，步驟如下:\n",
    "\n",
    "1. 使用selenium登入臉書，前往目標社團。\n",
    "2. 操作selenium將頁面不斷往下捲動。 (臉書的頁面必須不斷的往下拉，舊的內容才會顯示)\n",
    "3. 操作selenium將檢視另X留言、OOO已回覆 XX則回覆等，一一點開。\n",
    "4. 將現在頁面的html原始碼儲存下來。\n",
    "5. 使用beautifulsoup、re等套件解析原始碼，將誰在何時貼文與留言抓取下來。\n",
    "6. 如果某貼文或留言的時間在一個月內，則將回覆表情符號的超連結額外儲存起來。\n",
    "7. 使用selenium一一前往回覆表情符號的超連結，將其中所有的成員記錄下來。\n",
    "8. 將所有資料整併，輸出各成員在這段時間的社團餐與狀況。\n",
    "OK，那我們就來一一執行吧~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "登入facebook\n",
    "使用selenium開啟瀏覽器，前往臉書，輸入帳號密碼，點下登入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "profile = webdriver.FirefoxProfile() # 新增firefox的設定\n",
    "profile.set_preference(\"dom.webnotifications.enabled\", False) # 將頁面通知關掉\n",
    "profile.update_preferences() # 需要再更新目前firefox新的偏好設定\n",
    "driver = webdriver.Firefox(firefox_profile=profile)\n",
    "driver.get(\"http://www.facebook.com\")\n",
    "time.sleep(3)\n",
    "driver.find_element_by_id(\"email\").send_keys(USERNAME) # 將USERNAME改為你的臉書帳號\n",
    "driver.find_element_by_id(\"pass\").send_keys(PASSWORD) # 將PASSWORD改為你的臉書密碼\n",
    "driver.find_element_by_id(\"loginbutton\").click()\n",
    "time.sleep(3)\n",
    "driver.get('https://www.facebook.com/groups/733787316774129/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "由於示範的臉書社團是公開社團，因此可以省略登入，但如果目標是秘密社團，某先你必須先加入那個社團，然後這裡再登入。\n",
    "\n",
    "我額外加入了profile.set_preference(\"dom.webnotifications.enabled\", False)，這段firefox的設定，避免惱人的臉書通知訊息冒出來，阻擋住selenium下一步的動作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "捲動視窗與點開隱藏留言\n",
    "要將視窗往下捲，內容才會出現在網頁原始碼上。依照社團熱烈的程度，如果有很多貼文，就必需不斷往下捲，才能將一個月內的貼文都包含到裡面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12): # 捲動12次\n",
    "    driver.execute_script(\"window.scrollTo(0, {})\".format(4000 * (i + 1))) 每次捲動4000的單位\n",
    "    time.sleep(2) # 等待2秒鐘讓頁面讀取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "目標是一個月內的po文，依照你的目標社團去調整頁面捲動的次數。\n",
    "\n",
    "接著讓selenium將所有檢視另X留言、OOO已回覆 XX則回覆等，一一點開。\n",
    "\n",
    "有些按鈕是在第一次點開之後才會出現，因此需要重複執行兩輪。\n",
    "\n",
    "那麼，是需要定位到哪個元素去執行click呢?這裡就需要仔細的檢查元素，可以參考我的教學，如何檢查元素。\n",
    "\n",
    "根據我檢查的結果，目標位在<a test-id=\"UFI2CommentsPagerRenderer/pager_depth_0\"> 或 <a test-id=\"UFI2CommentsPagerRenderer/pager_depth_1\">底下，因此程式碼如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClickForMore():\n",
    "    hrefBtns = driver.find_elements_by_tag_name('a')    \n",
    "    for btn in hrefBtns:\n",
    "        try:\n",
    "            s = btn.get_attribute('data-testid')\n",
    "        except:\n",
    "            continue\n",
    "        if s == 'UFI2CommentsPagerRenderer/pager_depth_1' or s == 'UFI2CommentsPagerRenderer/pager_depth_0':\n",
    "            try:\n",
    "                btn.click()\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                continue\n",
    "ClickForMore()\n",
    "ClickForMore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "解析原始碼\n",
    "取得網站的原始碼，進行解析，我先使用beautifulsoup選取每則貼文及留言的區域，再以regex字串比對抓取時間和成員ID。原始碼中有unix格式的時間，我使用datetime來進行簡單的比對，並設定起始與結束日期來設定時間範圍。如果有表情符號回覆，則把連結網址留下來，需要以selenium再次進行爬取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "htmltext = driver.page_source # 將網頁原始碼拿出來\n",
    "\n",
    "\n",
    "def parse_htmltext(htmltext, start_date, end_date):\n",
    "    '''\n",
    "    解析臉書貼文與回覆的原始碼。\n",
    "    htmltext為原始碼，str\n",
    "    star_date為起始日期，datetime.datetime\n",
    "    end_date為結束日期，datetime.datetime\n",
    "    '''   \n",
    "    post_persons = []\n",
    "    comment_persons = []\n",
    "    good_urllist = [] # 回復表情符號超連結\n",
    "    ustart_date = start_date.timestamp()\n",
    "    uend_date = end_date.timestamp()\n",
    "    soup = BeautifulSoup(htmltext, 'html.parser')\n",
    "    body = soup.find('body')\n",
    "    posts = body.select('div[id=\"pagelet_group_mall\"]')[0].select('div[aria-label=\"動態消息\"]')[0]\n",
    "    feed_articles = posts.select('div[role=\"feed\"]')[0].select('div[role=\"article\"]')\n",
    "    other_articles = posts.select('div[role=\"article\"]')\n",
    "    articles = feed_articles + other_articles # 所有貼文或留言\n",
    "    \n",
    "    for article in articles:\n",
    "        if article.has_attr('id'):\n",
    "            try:\n",
    "                post_person = re.findall('title=\"(.{2,20})\"><div class=', str(article))[0]\n",
    "            except:\n",
    "                continue\n",
    "            post_time = int(re.findall('data-utime=\"(.*?)\"', str(article))[0])        \n",
    "            if post_time >= ustart_date and post_time <= uend_date:                \n",
    "                post_persons.append(post_person)\n",
    "            try:\n",
    "                good_urllist.append(re.findall('\"(/ufi/reaction/profile/browser/\\?.*?)\"', str(article))[0])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        elif article.has_attr('data-testid'):            \n",
    "            comment_person = re.findall('directed_target_id.*?href=\".*?\">(.*?)</a>', str(article))[0]  \n",
    "            comment_time = int(re.findall('data-utime=\"(.*?)\"', str(article))[0])\n",
    "            if comment_time >= ustart_date and post_time <= uend_date:                    \n",
    "                comment_persons.append(comment_person)                    \n",
    "                try:\n",
    "                    good_urllist.append(re.findall('\"(/ufi/reaction/profile/browser/\\?.*?)\"', str(article))[0])\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    return post_persons, comment_persons, good_urllist\n",
    "\n",
    "post_persons, comment_persons, good_urllist = parse_htmltext(htmltext, datetime.datetime(2019, 10, 15), datetime.datetime(2019, 11, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "臉書的原始碼很難解析，花了許多時間才成功爬取出來我要的資訊，細節我就不一一介紹，總之就是一再的察看原始碼的模式，然後想辦法用beautifulsoup選取到正確的區域，再以re去比對抓取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "再次抓取表情符號\n",
    "經我的測試，一定要登入臉書才能拜訪表成符號的連結。每個頁面的內容非常統一而且簡單，因此就直接進行解析。\n",
    "\n",
    "表情符號有很多種，但這裡就不一一區隔開來。如果你想知道誰按了幾次某種表情符號，則在這裡解析原始碼的地方設法將它區隔開來。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_good_urllist(urllist):\n",
    "    \n",
    "    output = []\n",
    "\n",
    "    profile = webdriver.FirefoxProfile()\n",
    "    profile.set_preference(\"dom.webnotifications.enabled\", False)\n",
    "    profile.update_preferences()\n",
    "    driver = webdriver.Firefox(firefox_profile=profile)\n",
    "    driver.get(\"http://www.facebook.com\")\n",
    "    driver.find_element_by_id(\"email\").send_keys(USERNAME) # 將USERNAME改為你的臉書帳號\n",
    "    driver.find_element_by_id(\"pass\").send_keys(PASSWORD) # 將PASSWORD改為你的臉書帳號\n",
    "    driver.find_element_by_id(\"loginbutton\").click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    for url in urllist:\n",
    "        driver.get('http://www.facebook.com/' + url)\n",
    "        htmltext = driver.page_source\n",
    "        soup = BeautifulSoup(htmltext, 'html.parser')\n",
    "        for raw_text in soup.select('li[class=\"_5i_q\"]'):\n",
    "            output += re.findall(re.compile('aria-label=\"(.*?)\" class=\"_s'),str(raw_text))            \n",
    "\n",
    "    driver.close()\n",
    "    return output\n",
    "\n",
    "emoji_replies = parse_good_urllist(good_urllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "整理數據，完工\n",
    "將前面產出的post_persons、comment_persons、emoji_persons，轉換成計次，再將結果打包成excel檔輸出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def tidy_up_data(post_persons, comment_persons, emoji_persons):\n",
    "    \n",
    "    all_persons = list(set(post_persons+comment_persons+emoji_persons))\n",
    "    post_times = []\n",
    "    comment_times = []\n",
    "    emoji_times = []\n",
    "    \n",
    "    for p in all_persons:\n",
    "        post_times.append(post_persons.count(p))\n",
    "        comment_times.append(comment_persons.count(p))\n",
    "        emoji_times.append(emoji_persons.count(p))\n",
    "    \n",
    "    return pd.DataFrame(dict(成員ID=all_persons, 貼文次數=post_times, 回文次數=comment_times, 回覆表情符號次數=emoji_times))\n",
    "        \n",
    "df = tidy_up_data(post_persons, comment_persons, emoji_persons)\n",
    "df.to_excel('member_activity.xlsx', index=False)\n",
    "#將最後結果輸出為member_activity.xlsx，輸出的資料如下:(只呈現部分資料，並將成員ID遮起來)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "結語\n",
    "恭喜你學會如何抓取臉書社團資料了~\n",
    "\n",
    "步驟很多，關關難過關關過，最後就能得的你要的結果。編寫爬蟲程式是需要時間慢慢磨的，從一再的檢查網頁，擬定爬取策略，到細部解析原始碼的正規表達式寫法，都是需要來來回回的嘗試。尤其是解析原始碼的部分，非常麻煩，你可以嘗試不用我的程式，自己慢慢編寫看看，說不定會比我的方式更加簡潔。\n",
    "\n",
    "另外，我原本擔心會不會因為臉書經常更新，而導致隔一陣子程式碼就無法作用，不過情況還好，在這一年內只有改動過一次小地方。如果到時候沒辦法運行的話，就再想辦法修復吧!\n",
    "\n",
    "下一篇我會介紹另外一種臉書爬蟲的應用，抓取照片，各位好好期待吧~\n",
    "\n",
    "感謝你的耐心閱讀到最後，希望你有學到些什麼，我們有緣再見~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "完整程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def get_htmltext(username, password):\n",
    "    '''\n",
    "    username 你的臉書帳號\n",
    "    password 你的臉書密碼\n",
    "    '''    \n",
    "    profile = webdriver.FirefoxProfile()\n",
    "    profile.set_preference(\"dom.webnotifications.enabled\", False)     \n",
    "    profile.update_preferences()     \n",
    "    driver = webdriver.Firefox(firefox_profile=profile)    \n",
    "    driver.get(\"http://www.facebook.com\")\n",
    "    time.sleep(3)\n",
    "    driver.find_element_by_id(\"email\").send_keys(username)\n",
    "    driver.find_element_by_id(\"pass\").send_keys(password)\n",
    "    driver.find_element_by_id(\"loginbutton\").click()\n",
    "    time.sleep(3)\n",
    "    driver.get('https://www.facebook.com/groups/733787316774129/')\n",
    "    time.sleep(3)\n",
    "    for i in range(12):\n",
    "        y = 4000 * (i + 1)\n",
    "        driver.execute_script(f\"window.scrollTo(0, {y})\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    def ClickForMore():\n",
    "        hrefBtns = driver.find_elements_by_tag_name('a')    \n",
    "        for btn in hrefBtns:\n",
    "            try:\n",
    "                s = btn.get_attribute('data-testid')\n",
    "            except:\n",
    "                continue\n",
    "            if s == 'UFI2CommentsPagerRenderer/pager_depth_1' or s == 'UFI2CommentsPagerRenderer/pager_depth_0':\n",
    "                try:\n",
    "                    btn.click()\n",
    "                    time.sleep(1)\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "    ClickForMore()\n",
    "    ClickForMore()      \n",
    "\n",
    "    htmltext = driver.page_source\n",
    "    driver.close()\n",
    "    \n",
    "    return htmltext\n",
    "\n",
    "def parse_htmltext(htmltext, start_date, end_date):\n",
    "    '''\n",
    "    解析臉書貼文與回覆的原始碼。\n",
    "    htmltext為原始碼，str\n",
    "    star_date為起始日期，datetime.datetime\n",
    "    end_date為結束日期，datetime.datetime\n",
    "    '''\n",
    "    post_persons = []\n",
    "    comment_persons = []\n",
    "    good_urllist = []\n",
    "    ustart_date = start_date.timestamp()\n",
    "    uend_date = end_date.timestamp()\n",
    "    soup = BeautifulSoup(htmltext, 'html.parser')\n",
    "    body = soup.find('body')\n",
    "    posts = body.select('div[id=\"pagelet_group_mall\"]')[0].select('div[aria-label=\"動態消息\"]')[0]\n",
    "    feed_articles = posts.select('div[role=\"feed\"]')[0].select('div[role=\"article\"]')\n",
    "    other_articles = posts.select('div[role=\"article\"]')\n",
    "    articles = feed_articles + other_articles\n",
    "    \n",
    "    for article in articles:\n",
    "        if article.has_attr('id'):\n",
    "            try:\n",
    "                post_person = re.findall('title=\"(.{2,20})\"><div class=', str(article))[0]\n",
    "            except:\n",
    "                continue\n",
    "            post_time = int(re.findall('data-utime=\"(.*?)\"', str(article))[0])        \n",
    "            if post_time >= ustart_date and post_time <= uend_date:                \n",
    "                post_persons.append(post_person)\n",
    "            try:\n",
    "                good_urllist.append(re.findall('\"(/ufi/reaction/profile/browser/\\?.*?)\"', str(article))[0])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "        elif article.has_attr('data-testid'):            \n",
    "            comment_person = re.findall('directed_target_id.*?href=\".*?\">(.*?)</a>', str(article))[0]  \n",
    "            comment_time = int(re.findall('data-utime=\"(.*?)\"', str(article))[0])\n",
    "            if comment_time >= ustart_date and post_time <= uend_date:                    \n",
    "                comment_persons.append(comment_person)                    \n",
    "                try:\n",
    "                    good_urllist.append(re.findall('\"(/ufi/reaction/profile/browser/\\?.*?)\"', str(article))[0])\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    return post_persons, comment_persons, good_urllist\n",
    "\n",
    "def parse_good_urllist(username, password,urllist):\n",
    "    '''\n",
    "    username 你的臉書帳號\n",
    "    password 你的臉書密碼\n",
    "    ''' \n",
    "    \n",
    "    output = []\n",
    "\n",
    "    profile = webdriver.FirefoxProfile()\n",
    "    profile.set_preference(\"dom.webnotifications.enabled\", False)  # Finally, turned off webnotifications...\n",
    "    profile.update_preferences()\n",
    "    driver = webdriver.Firefox(firefox_profile=profile)\n",
    "    driver.get(\"http://www.facebook.com\")\n",
    "    time.sleep(3)\n",
    "    driver.find_element_by_id(\"email\").send_keys(username)\n",
    "    driver.find_element_by_id(\"pass\").send_keys(password)\n",
    "    driver.find_element_by_id(\"loginbutton\").click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    for url in urllist:\n",
    "        driver.get('http://www.facebook.com/' + url)\n",
    "        htmltext = driver.page_source\n",
    "        soup = BeautifulSoup(htmltext, 'html.parser')\n",
    "        for raw_text in soup.select('li[class=\"_5i_q\"]'):\n",
    "            output += re.findall(re.compile('aria-label=\"(.*?)\" class=\"_s'),str(raw_text))            \n",
    "\n",
    "    driver.close()\n",
    "    return output\n",
    "\n",
    "\n",
    "def tidy_up_data(post_persons, comment_persons, emoji_persons):\n",
    "    \n",
    "    all_persons = list(set(post_persons+comment_persons+emoji_persons))\n",
    "    post_times = []\n",
    "    comment_times = []\n",
    "    emoji_times = []\n",
    "    \n",
    "    for p in all_persons:\n",
    "        post_times.append(post_persons.count(p))\n",
    "        comment_times.append(comment_persons.count(p))\n",
    "        emoji_times.append(emoji_persons.count(p))\n",
    "    \n",
    "    return pd.DataFrame(dict(成員ID=all_persons, 貼文次數=post_times, 回文次數=comment_times, 回覆表情符號次數=emoji_times))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    username = 'YOUR USERNAME'\n",
    "    password = 'YOUR PASSWORD'\n",
    "\n",
    "    htmltext = get_htmltext(username, password)\n",
    "    post_persons, comment_persons, good_urllist = parse_htmltext(htmltext, datetime.datetime(2019,10,15), datetime.datetime(2019,11,15))\n",
    "    emoji_persons = parse_good_urllist(username, password, good_urllist)\n",
    "    df = tidy_up_data(post_persons, comment_persons, emoji_persons)\n",
    "    df.to_excel('member_activity.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
