{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Finance 個股資訊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這隻要細心一點, 除了會把指定編號的股票資訊顯示出來之外, 另外也實作了取得不同頁歷史資訊的方法:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'span'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-525491b74ce3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-525491b74ce3>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mfinance_info_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_web_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGOOGLE_FINANCE_INFO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCODE_TSMC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinance_info_page\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mstock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_stock_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinance_info_page\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\":\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-525491b74ce3>\u001b[0m in \u001b[0;36mget_stock_info\u001b[1;34m(dom)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mstock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mstock\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mstock\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'current_price'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'price-panel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mstock\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'current_change'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'price-panel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'id-price-change'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'snap-panel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'span'"
     ]
    }
   ],
   "source": [
    "#https://www.google.com/finance?q=TPE:2330網址和標籤根本不同???\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# To query the specific stock info, add \"MARKET:STOCK_ID\" after the url.\n",
    "# e.g.: https://www.google.com/finance?q=[MARKET]:[STOCK_ID]\n",
    "GOOGLE_FINANCE_INFO = \"https://www.google.com/finance?q=\"\n",
    "GOOGLE_FINANCE_HISTORY = \"https://www.google.com/finance/historical?q=\"\n",
    "CODE_TSMC = \"TPE:2330\"\n",
    "FINANCE_HISTORY_START_BASE = 30\n",
    "FINANCE_HISTORY_RECORD_SIZE = 30\n",
    "\n",
    "\n",
    "def get_web_page(url, query):\n",
    "    if query:\n",
    "        resp = requests.get(url+query)\n",
    "    else:\n",
    "        resp = requests.get(url)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url: ', resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        return resp.text\n",
    "\n",
    "\n",
    "def get_stock_info(dom):\n",
    "    soup = BeautifulSoup(dom, 'html5lib')\n",
    "    stock = dict()\n",
    "    stock['name'] = soup.title.text.split(':')[0]\n",
    "    stock['current_price'] = soup.find(id='price-panel').span.span.text\n",
    "    stock['current_change'] = soup.find(id='price-panel').find('div', 'id-price-change').text.strip().replace('\\n', '')\n",
    "    for table in soup.find('div', 'snap-panel').find_all('table'):\n",
    "        for tr in table.find_all('tr'):\n",
    "            key = tr.find('td', 'key').text.lower().strip()\n",
    "            value = tr.find('td', 'val').text.strip()\n",
    "            stock[key] = value\n",
    "    return stock\n",
    "\n",
    "\n",
    "def get_stock_history(dom):\n",
    "    soup = BeautifulSoup(dom, 'html5lib')\n",
    "    table = soup.find('table', 'historical_price')\n",
    "    header_row = table.find('tr', 'bb')\n",
    "    headers = [header for header in header_row.stripped_strings]\n",
    "    print(headers)\n",
    "    for tds in table.find_all('tr')[1:]:\n",
    "        print([data for data in tds.stripped_strings])\n",
    "\n",
    "\n",
    "def main():\n",
    "    finance_info_page = get_web_page(GOOGLE_FINANCE_INFO, CODE_TSMC)\n",
    "    if finance_info_page:\n",
    "        stock = get_stock_info(finance_info_page)\n",
    "        for k, v in stock.items():\n",
    "            print(k + \":\", v)\n",
    "\n",
    "    # To fetch history data by pagination, you need to add two query params: start and num.\n",
    "    # Just like the following link:\n",
    "    # https://www.google.com/finance/historical?q=TPE:2330&start=30&num=30\n",
    "    for page in range(0, 3, 1):\n",
    "        history_url = GOOGLE_FINANCE_HISTORY + CODE_TSMC + \"&start=\" + str(page * FINANCE_HISTORY_START_BASE) + \"&num=\" + str(FINANCE_HISTORY_RECORD_SIZE)\n",
    "        finance_history_page = get_web_page(history_url, None)\n",
    "        get_stock_history(finance_history_page)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-553ac006552c>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-553ac006552c>\"\u001b[1;36m, line \u001b[1;32m20\u001b[0m\n\u001b[1;33m    stock['name'] = sections[1].div.text\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# 網址後方加上 MARKET:STOCK_ID 即為個股資訊. e.g, TPE:2330\n",
    "G_FINANCE_URL = 'https://www.google.com/search?q='\n",
    "def get_web_page(url, stock_id):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "                             'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                             'Chrome/66.0.3359.181 Safari/537.36'}\n",
    "    resp = requests.get(url + stock_id, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url:', resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        return resp.text\n",
    "def get_stock_info(dom):\n",
    "    soup = BeautifulSoup(dom, 'html5lib')\n",
    "    stock = dict()\n",
    "sections = soup.find_all('g-card-section')\n",
    "# 第 2 個 g-card-section, 取出公司名及即時股價資訊\n",
    "    stock['name'] = sections[1].div.text\n",
    "    spans = sections[1].find_all('div', recursive=False)[1].find_all('span', recursive=False)\n",
    "    stock['current_price'] = spans[0].text\n",
    "    stock['current_change'] = spans[1].text\n",
    "# 第 4 個 g-card-section, 有左右兩個 table 分別存放股票資訊\n",
    "    for table in sections[3].find_all('table'):\n",
    "        for tr in table.find_all('tr')[:3]:\n",
    "            key = tr.find_all('td')[0].text.lower().strip()\n",
    "            value = tr.find_all('td')[1].text.strip()\n",
    "            stock[key] = value\n",
    "return stock\n",
    "if __name__ == '__main__':\n",
    "    page = get_web_page(G_FINANCE_URL, 'TPE:2330')\n",
    "    if page:\n",
    "        stock = get_stock_info(page)\n",
    "        for k, v in stock.items():\n",
    "            print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取Yahoo股票資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ch9_2(yahoo_stock_crawler.py)-爬取Yahoo股票資訊\n",
    "import time\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#目標URL網址，s=股票代碼\n",
    "URL = \"https://tw.stock.yahoo.com/q/q?s=\"\n",
    "\n",
    "#使用for迴圈建立回傳的URL清單，也就是在基底網址的最後加上股票代碼的s參數值\n",
    "def generate_urls(url, stocks):\n",
    "    urls = []  #爬蟲主程式建立的目標網址清單\n",
    "    for stock in stocks:\n",
    "        urls.append(url + stock)\n",
    "    return urls\n",
    "\n",
    "#使用requests物件，以自訂HTTP標頭來送出HTTP請求\n",
    "def get_resource(url):\n",
    "    headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "               \"AppleWebKit/537.36 (KHTML, like Gecko)\"\n",
    "               \"Chrome/63.0.3239.132 Safari/537.36\"}\n",
    "    return requests.get(url, headers=headers) \n",
    "\n",
    "def parse_html(html_str):\n",
    "    return BeautifulSoup(html_str, \"lxml\")\n",
    "\n",
    "#用find_all函數找出第一個表格的HTML標籤後，使用select函數以CSS選擇器取出指定儲存格的股票資料\n",
    "def get_stock(soup, stock_id):\n",
    "    table = soup.find_all(text=\"成交\")[0].parent.parent.parent\n",
    "    status = table.select(\"tr\")[0].select(\"th\")[2].text\n",
    "    name =   table.select(\"tr\")[1].select(\"td\")[0].text\n",
    "    price =  table.select(\"tr\")[1].select(\"td\")[2].text\n",
    "    yclose = table.select(\"tr\")[1].select(\"td\")[7].text\n",
    "    volume = table.select(\"tr\")[1].select(\"td\")[6].text\n",
    "    high =   table.select(\"tr\")[1].select(\"td\")[9].text\n",
    "    low  =   table.select(\"tr\")[1].select(\"td\")[10].text\n",
    "    \n",
    "    return [stock_id, name[4:-6], status, price, yclose, volume, high, low]  \n",
    "    #name[4:-6] : 為將\"3711日月光投控加到投資組合\"前5~後7字串只取出\"日月光投控\"\n",
    "\n",
    "#將Python巢狀清單輸出存成CSV檔\n",
    "def save_to_csv(items, file):\n",
    "    with open(file, \"w+\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        for item in items:\n",
    "            writer.writerow(item)\n",
    "\n",
    "#用來爬取股票資料，因為是URL清單，所以使用for迴圈來一一爬取每個URL，首先使用split函數取得股票代碼stcok_id\n",
    "def web_scraping_bot(urls):\n",
    "    stocks = [[\"代碼\",\"名稱\",\"狀態\",\"股價\",\"昨收\",\"張數\",\"最高\",\"最低\"]]  #此為巢狀清單\n",
    "    \n",
    "    for url in urls:\n",
    "        stock_id = url.split(\"=\")[-1] #用\"=\"分割url，並取最後一個，就是股票代碼\n",
    "        print(\"抓取: \" + stock_id + \" 網路資料中...\")\n",
    "        r = get_resource(url) #送出HTTP請求\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            soup = parse_html(r.text) #請求成功的話就用美湯剖析網頁\n",
    "            stock = get_stock(soup, stock_id) #取得這一檔股票資訊(def 4)\n",
    "            stocks.append(stock) #將參數股票資訊清單新增至巢狀清單\n",
    "            print(\"等待5秒鐘...\")\n",
    "            time.sleep(5) \n",
    "        else:\n",
    "            print(\"HTTP請求錯誤...\")       \n",
    "\n",
    "    return stocks\n",
    "\n",
    "#爬蟲主程式\n",
    "if __name__ == \"__main__\":  #當被匯入時不執行的段落\n",
    "#__name__ 是「當前模組名稱」，可以用來分辨程式是被直接執行還是被import的，當模組被直接執行時模組名為 __main__ 。\n",
    "#這句話的意思就是，當模組被直接執行時，之下的程式將被執行，當模組是被匯入時，程式碼塊不被執行。\n",
    "#是因為程式在匯入時都會重覆執行匯入的程式，但加入這段就可以避免某些段落被重覆執行。\n",
    "    urls = generate_urls(URL, [\"3711\", \"2330\", \"2454\",\"2365\"]) #generate_urls函數建立目標網址清單，第一個參數是基底URL，第二個是股票代碼清單\n",
    "    print(urls)\n",
    "    stocks = web_scraping_bot(urls) #web_scraping_bot函數以參數的URL清單來爬取資料，回傳的是各檔股票的資訊\n",
    "    for stock in stocks:\n",
    "        print(stock)\n",
    "    save_to_csv(stocks, \"stocks.csv\") #存成CSV檔的函數"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
