{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ch9_2(nba_player_crawler.py)-爬取NBA球隊的陣容\n",
    "import time\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 目標URL網址\n",
    "URL = \"https://www.basketball-reference.com/teams/{0}/2020.html\"\n",
    "TEAMS = [\"LAL\", \"HOU\", \"GSW\"]\n",
    "\n",
    "def generate_urls(url, teams):\n",
    "    urls = []\n",
    "    for team in teams:\n",
    "        urls.append(url.format(team))\n",
    "    return urls\n",
    "\n",
    "def get_resource(url):\n",
    "    headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "               \"AppleWebKit/537.36 (KHTML, like Gecko)\"\n",
    "               \"Chrome/63.0.3239.132 Safari/537.36\"}\n",
    "    return requests.get(url, headers=headers) \n",
    "\n",
    "def parse_html(html_str):\n",
    "    return BeautifulSoup(html_str, \"lxml\")\n",
    "\n",
    "def get_players(soup, team):\n",
    "    team_players = []\n",
    "    table = soup.find(id=\"roster\")  # 找到表格\n",
    "    # HTML表格的所有列\n",
    "    for row in table.find(\"tbody\").find_all(\"tr\"): \n",
    "        no = row.th.text  # 背號\n",
    "        cols = row.findAll(\"td\")\n",
    "        # 球隊, 背號, 姓名, 位置, 體重, 生日, 經驗, 大學\n",
    "        team_players.append([team, no, cols[0].text, cols[1].text, \n",
    "                             cols[3].text, cols[4].text, \n",
    "                             cols[6].text, cols[7].text])\n",
    "            \n",
    "    return team_players\n",
    "\n",
    "def save_to_csv(items, file):\n",
    "    with open(file, \"w+\", newline=\"\", encoding=\"utf-8\") as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        for item in items:\n",
    "            writer.writerow(item)\n",
    "\n",
    "def web_scraping_bot(urls):\n",
    "    count = 0    \n",
    "    total_players=[[\"球隊\",\"背號\",\"姓名\",\"位置\",\"體重\",\"生日\",\"經驗\",\"大學\"]]\n",
    "    \n",
    "    for url in urls:\n",
    "        team_name = TEAMS[count]\n",
    "        count = count + 1;\n",
    "        print(\"抓取: \" + team_name + \" 網路資料中...\")\n",
    "        r = get_resource(url)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            soup = parse_html(r.text)\n",
    "            players = get_players(soup, team_name)\n",
    "            total_players = total_players + players\n",
    "            print(\"等待5秒鐘...\")\n",
    "            time.sleep(5)            \n",
    "        else:\n",
    "            print(\"HTTP請求錯誤...\")       \n",
    "\n",
    "    return total_players\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls = generate_urls(URL, TEAMS)\n",
    "    # print(urls)\n",
    "    players = web_scraping_bot(urls)\n",
    "    for item in players:\n",
    "        print(item)\n",
    "    save_to_csv(players, \"players.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
