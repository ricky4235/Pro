{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTT八卦板今日熱門文章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PTT web版的html結構算是比較有規則的, 所以也是拿來練爬蟲的好對象, 下面這隻爬蟲的目的是要去找出今日的熱門文章(50推以上), 同時也會去找出今天有哪些5566發文了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's 5566:\n",
      "{'RS5566', 'Bonker5566', 'Rin5566', 'XDDDpupu5566', 'fun5566', 'lianpig5566', 'kameaki5566', 'stock5566', 'nikubou5566', 'Nigger5566', 'scum5566', 'zrct5566', 'Andy5566', 'gangster5566', 'purine5566', 'TomFord5566', 'junkjizz5566', 'laba5566', 'youtu5566', 'dickhole5566', 'fantacy5566', 'ce3255666', 'zyc5566', 'WOWO5566', 'vc5566', 'ARZT5566', 'aass5566', 'PA5566', 'AKB5566'}\n",
      "\n",
      "There are  1002  posts today.\n",
      "Hot post(≥ 50 push): \n",
      "{'title': '[爆卦] 台灣民眾黨-要求陸委會「講清楚說明白」!', 'href': '/bbs/Gossiping/M.1581478722.A.8B2.html', 'push_count': 99, 'author': 'PunkGrass'}\n",
      "{'title': 'Re: [問卦] 有小明之亂的懶人包嗎？', 'href': '/bbs/Gossiping/M.1581476703.A.395.html', 'push_count': 99, 'author': 'terry1020'}\n",
      "{'title': '[新聞] WHO命名COVID-19 我國維持簡稱\"武漢肺炎\"', 'href': '/bbs/Gossiping/M.1581476283.A.98D.html', 'push_count': 99, 'author': 'thouloveme'}\n",
      "{'title': '[新聞] 入場泳客從寶瓶星號下船才1天 花蓮業者', 'href': '/bbs/Gossiping/M.1581475689.A.36C.html', 'push_count': 99, 'author': 'stevenchiang'}\n",
      "{'title': '[新聞] 網路瘋傳仁寶工程師疑似感染武漢肺炎 仁', 'href': '/bbs/Gossiping/M.1581474983.A.8F5.html', 'push_count': 54, 'author': 'smalltwo'}\n",
      "{'title': '[新聞] 「你先領」卻自己戴口罩遭批 林楚茵：防', 'href': '/bbs/Gossiping/M.1581475498.A.D64.html', 'push_count': 99, 'author': 'SAgirl'}\n",
      "{'title': '[新聞] 掃光京都口罩！陸女大賺500萬災難財', 'href': '/bbs/Gossiping/M.1581474178.A.B0E.html', 'push_count': 85, 'author': 'SuperSg'}\n",
      "{'title': '[新聞] 汐止機車追撞左轉賓士車 騎士為超車送醫', 'href': '/bbs/Gossiping/M.1581473427.A.616.html', 'push_count': 64, 'author': 'Aqqqa'}\n",
      "{'title': '[新聞] 出現新冠肺炎病徵 鍾南山呼籲：不要在家隔離', 'href': '/bbs/Gossiping/M.1581472750.A.A47.html', 'push_count': 99, 'author': 'currykukuo'}\n",
      "{'title': '[爆卦] 時力出聲了-呼籲陸委會', 'href': '/bbs/Gossiping/M.1581471316.A.FBB.html', 'push_count': 99, 'author': 'octopus4406'}\n",
      "{'title': '[爆卦] 為搶消毒水17歲女持刀殺傷7旬婦9歲幼女', 'href': '/bbs/Gossiping/M.1581471601.A.B42.html', 'push_count': 56, 'author': 'nomorepipe'}\n",
      "{'title': '[爆卦] DPP 2020紀念飛行夾克 開賣啦！', 'href': '/bbs/Gossiping/M.1581469691.A.E2B.html', 'push_count': 91, 'author': 'currykukuo'}\n",
      "{'title': '[問卦] 台灣人有可能跟中國大陸人做朋友嗎？', 'href': '/bbs/Gossiping/M.1581468257.A.1BE.html', 'push_count': 75, 'author': 'parttime'}\n",
      "{'title': '[新聞] 快訊／「鑽石公主號」再39例確診！累積17', 'href': '/bbs/Gossiping/M.1581468365.A.AEC.html', 'push_count': 81, 'author': 'osuki'}\n",
      "{'title': '[新聞] 習近平警告防疫太過損害經濟 國務院下令', 'href': '/bbs/Gossiping/M.1581467581.A.B21.html', 'push_count': 98, 'author': 'z770808'}\n",
      "{'title': '[爆卦] 周玉蔻說小明一共有千餘人', 'href': '/bbs/Gossiping/M.1581468044.A.EBA.html', 'push_count': 99, 'author': 'IronCube'}\n",
      "{'title': '[爆卦] 鑽石公主號新增39例', 'href': '/bbs/Gossiping/M.1581467016.A.C40.html', 'push_count': 60, 'author': 'BURNFISH'}\n",
      "{'title': '[問卦] FB的817好像都無聲無息了？', 'href': '/bbs/Gossiping/M.1581467400.A.F6A.html', 'push_count': 69, 'author': 'JeremyYi'}\n",
      "{'title': '[問卦] 怎麼讓政府知道「他小明 我不ok」?', 'href': '/bbs/Gossiping/M.1581466286.A.04B.html', 'push_count': 54, 'author': 'FD56'}\n",
      "{'title': '[爆卦] 川普提案將美國對WHO的金援砍半', 'href': '/bbs/Gossiping/M.1581465006.A.C07.html', 'push_count': 75, 'author': 'Scion'}\n",
      "{'title': '[爆卦] 鑽石公主號再加39人', 'href': '/bbs/Gossiping/M.1581465118.A.27F.html', 'push_count': 99, 'author': 'xxLolaxx'}\n",
      "{'title': '[問卦] qn什麼時候變成政治評論員了?', 'href': '/bbs/Gossiping/M.1581462984.A.F2F.html', 'push_count': 99, 'author': 'SCEW'}\n",
      "{'title': '[新聞] 馬拉威法院宣布民進黨總統當選無效', 'href': '/bbs/Gossiping/M.1581463331.A.883.html', 'push_count': 67, 'author': 'a10141013'}\n",
      "{'title': '[爆卦] 44404確診 1112死亡', 'href': '/bbs/Gossiping/M.1581463984.A.288.html', 'push_count': 81, 'author': 'eddisontw'}\n",
      "{'title': '[新聞] 豬價跌破10年來低價大關 市場：供過於求', 'href': '/bbs/Gossiping/M.1581464391.A.508.html', 'push_count': 92, 'author': 'loserfatotak'}\n",
      "{'title': '[問卦] 真的會有父母把小孩丟在中國?', 'href': '/bbs/Gossiping/M.1581461147.A.83D.html', 'push_count': 52, 'author': 'jeffery95099'}\n",
      "{'title': '[問卦] 約克羊畫的圖好可愛', 'href': '/bbs/Gossiping/M.1581452709.A.003.html', 'push_count': 62, 'author': 'david0426'}\n",
      "{'title': 'Re: [新聞] 陸配子女返台政策一日三變 陸委會再', 'href': '/bbs/Gossiping/M.1581453126.A.030.html', 'push_count': 99, 'author': 'sabinetw'}\n",
      "{'title': 'Re: [問卦] 第一波包機名單政府真的不知情？', 'href': '/bbs/Gossiping/M.1581455456.A.872.html', 'push_count': 59, 'author': 'aeolus811tw'}\n",
      "{'title': 'Re: [問卦] 陸委會事件，我有些話想說', 'href': '/bbs/Gossiping/M.1581451951.A.534.html', 'push_count': 56, 'author': 'Longchamp'}\n",
      "{'title': '[問卦] 為什麼港仔要這麼討人厭?', 'href': '/bbs/Gossiping/M.1581448597.A.9DB.html', 'push_count': 98, 'author': 'XSR700'}\n",
      "{'title': '[問卦] 必須為陸委會說些話', 'href': '/bbs/Gossiping/M.1581447291.A.94F.html', 'push_count': 99, 'author': 'pk698326889'}\n",
      "{'title': '[問卦] 會不會開放小明一天然後又轉彎', 'href': '/bbs/Gossiping/M.1581448025.A.047.html', 'push_count': 99, 'author': 'sulabird'}\n",
      "{'title': 'Re: [問卦] 陸委會一事  國民黨怎沒人出來罵?', 'href': '/bbs/Gossiping/M.1581444048.A.04C.html', 'push_count': 99, 'author': 'panzer1224'}\n",
      "{'title': 'Fw: [新聞] 陸委會新聞稿-024-陸配子女入境管制政策之說明', 'href': '/bbs/Gossiping/M.1581442913.A.13F.html', 'push_count': 84, 'author': 'fragmentwing'}\n",
      "{'title': '[問卦] 小明強制隔離14天可以接受嗎?', 'href': '/bbs/Gossiping/M.1581440393.A.1CB.html', 'push_count': 56, 'author': 'marijnkops'}\n",
      "{'title': '[新聞] 曾放話在機師餐加料！郭芷嫣遭長榮解雇', 'href': '/bbs/Gossiping/M.1581439395.A.30E.html', 'push_count': 99, 'author': 'karen8097'}\n",
      "{'title': '[問卦] COVID-19怎唸', 'href': '/bbs/Gossiping/M.1581439484.A.332.html', 'push_count': 68, 'author': 'dododada'}\n",
      "{'title': '[爆卦] 英國最新兩例可能在監獄裡面發現！', 'href': '/bbs/Gossiping/M.1581438343.A.494.html', 'push_count': 93, 'author': 'jimmylily'}\n",
      "{'title': 'Re: [新聞] 陸配子女返台政策一日三變 陸委會再限制', 'href': '/bbs/Gossiping/M.1581438463.A.E82.html', 'push_count': 99, 'author': 'zball'}\n",
      "{'title': '[問卦] 今晚很多關鍵字的出現頻率異常？？', 'href': '/bbs/Gossiping/M.1581438139.A.5CE.html', 'push_count': 54, 'author': 'offstage'}\n",
      "{'title': 'Re: [新聞] 陸配子女返台政策一日三變 陸委會再限制', 'href': '/bbs/Gossiping/M.1581438211.A.9E0.html', 'push_count': 99, 'author': 'dodobaho'}\n",
      "{'title': '[問卦] 有沒有身邊覺青都消失了的八卦', 'href': '/bbs/Gossiping/M.1581437517.A.122.html', 'push_count': 99, 'author': 'IDLONG'}\n",
      "{'title': '[爆卦] 整點啦 小明可以準備了', 'href': '/bbs/Gossiping/M.1581436841.A.8F0.html', 'push_count': 87, 'author': 'paralux'}\n",
      "{'title': '[問卦] 第一波包機名單政府真的不知情？', 'href': '/bbs/Gossiping/M.1581437036.A.08D.html', 'push_count': 92, 'author': 'reppoc'}\n",
      "------------------------\n",
      "[] /bbs/Gossiping/index39175.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc'\n",
    "\n",
    "\n",
    "def get_web_page(url):\n",
    "    resp = requests.get(url=url, cookies={'over18': '1'})\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url:', resp.url)\n",
    "        return None  #有異常的話回傳None\n",
    "    else:\n",
    "        return resp.text #成功的話回傳網頁資訊\n",
    "\n",
    "\n",
    "def get_articles(dom, date):\n",
    "    soup = BeautifulSoup(dom, 'html5lib')  #\"dom\"是main()爬蟲主程式中的輸入值\n",
    "    # 取得該頁面上一頁連結（因為爬完原頁資料後就要往前爬舊的資料，所以過程中要不斷往上一頁）\n",
    "    paging_div = soup.find('div', 'btn-group btn-group-paging')  #換頁區塊標籤位置\n",
    "    prev_url = paging_div.find_all('a')[1]['href']   #<a>的第2個'href'為'上一頁'的連結\n",
    "\n",
    "    articles = []  # 用來儲存文章資料\n",
    "    divs = soup.find_all('div', 'r-ent')  #文章區塊標籤\n",
    "    for d in divs:\n",
    "        # If post date matched:\n",
    "        if d.find('div', 'date').text.strip() == date:  #\"date\"是main()爬蟲主程式中的輸入值\n",
    "            # 取得推文數\n",
    "            push_count = 0\n",
    "            push_str = d.find('div', 'nrec').text\n",
    "            if push_str:\n",
    "                try:\n",
    "                    push_count = int(push_str) #轉換字串為數字\n",
    "                except ValueError:\n",
    "                    # 若失敗，可能是有 '爆', 'X1', 'X2',....\n",
    "                    # 若不是，不做任何事，push_count保持為0\n",
    "                    if push_str == '爆':\n",
    "                        push_count = 99\n",
    "                    elif push_str.startswith('X'):  #startswith() :檢查字符串是否是以指定子字符串開頭，如果是則返回True否則False\n",
    "                        push_count = -10\n",
    "\n",
    "            # 取得文章連結和標題\n",
    "            if d.find('a'):\n",
    "                href = d.find('a')['href']\n",
    "                title = d.find('a').text\n",
    "                author = d.find('div', 'author').text if d.find('div', 'author') else ''  #後面看不懂\n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'href': href,\n",
    "                    'push_count': push_count,\n",
    "                    'author': author\n",
    "                })\n",
    "\n",
    "    return articles, prev_url   #回傳文章list，和這頁的\"上一頁\"超連結\n",
    "                                # 顯示為: [] /bbs/Gossiping/index39175.html\n",
    "\n",
    "\n",
    "def get_author_ids(posts, pattern):  #只取作者id資訊\n",
    "    ids = set()  #set() 函數創建一個無序不重複元素集，可進行關係測試，刪除重複數據，還可以計算交集、差集、並集等\n",
    "    for post in posts:  #main()中的posts=articles，也就是前面的文章資料\n",
    "        if pattern in post['author']:  #假如pattern再articles資料的['author']中\n",
    "            ids.add(post['author'])   #就加入ids這個集合中\n",
    "    return ids\n",
    "\n",
    "\n",
    "def main():  #爬蟲主程式（只要抓今日的文章）\n",
    "    current_page = get_web_page(PTT_URL + '/bbs/Gossiping/index.html') #先取得最新一頁的網頁\n",
    "    if current_page:  #如果取得成功的話\n",
    "        articles = [] #全部的今日文章\n",
    "        today = time.strftime(\"%m/%d\").lstrip('0')  #今天的日期，在這裡我們刪除開頭的0以匹配PTT日期的格式\n",
    "                                    # strftime() : http://tw.gitbook.net/python/time_strftime.html\n",
    "                                    # lstrip() : http://tw.gitbook.net/python/string_lstrip.html\n",
    "        current_articles, prev_url = get_articles(current_page, today) #目前頁面的今日文章\n",
    "        #current_articles:這頁的所有文章，prev_url這頁的\"上一頁\"超連結\n",
    "        while current_articles: #若目前頁面有今日文章則加入articles，並回到上一頁尋找是否有今日文章\n",
    "            articles += current_articles  #更新值 (就地加) ，每執行一次，articles = []就新增current_articles至articles\n",
    "            current_page = get_web_page(PTT_URL + prev_url) #到\"上一頁\"取得網址\n",
    "            current_articles, prev_url = get_articles(current_page, today) #再用\"上一頁\"的網址資訊取得今天的文章\n",
    "            #while current_articles:就繼續迴圈往上一頁找今日文章，直到上一頁沒有今日文章，也就是current_articles空白的時候回圈就停止\n",
    "        print(\"Today's 5566:\")\n",
    "        print(get_author_ids(articles, '5566'))\n",
    "\n",
    "        print('\\nThere are ', len(articles), ' posts today.')\n",
    "        threshold = 50   #threshold 門檻訂為50\n",
    "        print('Hot post(≥ %d push): ' % threshold)  #正則表達式-顯示為: \"Hot post(≥ 50 push):\" \n",
    "        for article in articles:\n",
    "            if int(article['push_count']) > threshold:  #列出推數大於門檻的文章\n",
    "                print(article)\n",
    "                \n",
    "        # with as: https://openhome.cc/Gossip/Python/WithAs.html\n",
    "        # json.dump: http://python3-cookbook.readthedocs.io/zh_CN/latest/c06/p02_read-write_json_data.html\n",
    "        with open('gossiping.json', 'w', encoding='UTF-8') as file:\n",
    "            json.dump(articles, file, indent=2, sort_keys=True, ensure_ascii=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-2c6597472504>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "a=2\n",
    "b=\"b\"\n",
    "a,b = 3, 9,8\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTT表特版下載器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beauty from: {'title': '[帥哥] Vince Carter', 'href': '/bbs/Beauty/M.1583988574.A.9A9.html', 'push_count': 63, 'author': 'Boss741108'}\n",
      "Collecting beauty from: {'title': '[帥哥] 福山雅治', 'href': '/bbs/Beauty/M.1583988635.A.F0B.html', 'push_count': 3, 'author': 'Gary5566'}\n",
      "Collecting beauty from: {'title': '[正妹] 大尺碼女孩(20)', 'href': '/bbs/Beauty/M.1583989363.A.253.html', 'push_count': 4, 'author': 'ckpot'}\n",
      "Collecting beauty from: {'title': '[新聞] 「屁孩殺手」女警背影網熱議跪求長相！', 'href': '/bbs/Beauty/M.1583990101.A.6C5.html', 'push_count': 0, 'author': 'KKKKJAY'}\n",
      "Collecting beauty from: {'title': '[正妹] 兇 年輕的岩間香須美', 'href': '/bbs/Beauty/M.1583993459.A.1B9.html', 'push_count': 43, 'author': 'AmedRosario'}\n",
      "Collecting beauty from: {'title': '[神人] 最右邊是誰阿', 'href': '/bbs/Beauty/M.1584006600.A.760.html', 'push_count': 16, 'author': 'QYIN712'}\n",
      "Collecting beauty from: {'title': '[正妹] 雲科大學生', 'href': '/bbs/Beauty/M.1584008037.A.7BC.html', 'push_count': 97, 'author': 'a3268403'}\n",
      "Collecting beauty from: {'title': '[正妹] 安芝', 'href': '/bbs/Beauty/M.1584011038.A.CF7.html', 'push_count': 3, 'author': 'haohao1201'}\n",
      "Collecting beauty from: {'title': '[神人] 部長後面那男的', 'href': '/bbs/Beauty/M.1584014478.A.0C4.html', 'push_count': 1, 'author': 'pian0214'}\n",
      "Collecting beauty from: {'title': '[正妹] TWICE x Qoo', 'href': '/bbs/Beauty/M.1584017293.A.C07.html', 'push_count': 3, 'author': 'fidelity77'}\n",
      "Collecting beauty from: {'title': '[帥哥] 真田廣之', 'href': '/bbs/Beauty/M.1583946241.A.C57.html', 'push_count': 14, 'author': 'XXXXPOXXXX'}\n",
      "Collecting beauty from: {'title': '[正妹] 混血', 'href': '/bbs/Beauty/M.1583946287.A.5BE.html', 'push_count': 6, 'author': 'Arschloch'}\n",
      "Collecting beauty from: {'title': '[正妹] 喝水解渴', 'href': '/bbs/Beauty/M.1583948329.A.0F7.html', 'push_count': 18, 'author': 'F1060113'}\n",
      "Collecting beauty from: {'title': '[神人] 很久的超商廣告女主角', 'href': '/bbs/Beauty/M.1583952998.A.D76.html', 'push_count': 2, 'author': 'd86249'}\n",
      "Collecting beauty from: {'title': '[正妹] 高中生', 'href': '/bbs/Beauty/M.1583958075.A.0A9.html', 'push_count': 34, 'author': 'd86249'}\n",
      "Collecting beauty from: {'title': '[正妹] 30歲的溜溜球不良少女', 'href': '/bbs/Beauty/M.1583963216.A.86A.html', 'push_count': 12, 'author': 'Wardyal'}\n",
      "Collecting beauty from: {'title': '[公告] 水桶', 'href': '/bbs/Beauty/M.1583970496.A.7D6.html', 'push_count': -10, 'author': 'hateOnas'}\n",
      "Collecting beauty from: {'title': '[廣告] AV界絕美女神 天海翼(天海つばさ)', 'href': '/bbs/Beauty/M.1583976024.A.7FF.html', 'push_count': 42, 'author': 'kelseyaya'}\n",
      "Collecting beauty from: {'title': '[神人] 抖音影片的女主角', 'href': '/bbs/Beauty/M.1583980460.A.DEF.html', 'push_count': 0, 'author': 'QchchQ'}\n",
      "Collecting beauty from: {'title': '[正妹] 不偷的F奶', 'href': '/bbs/Beauty/M.1583984758.A.8B4.html', 'push_count': -10, 'author': 'gto3ping'}\n",
      "Collecting beauty from: {'title': '[公告] 本人在職時 新版主的限制', 'href': '/bbs/Beauty/M.1583986287.A.723.html', 'push_count': -10, 'author': 'hateOnas'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc'\n",
    "\n",
    "\n",
    "def get_web_content(url):\n",
    "    resp = requests.get(url=url, cookies={'over18': '1'})\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url: ' + resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        return resp.text\n",
    "\n",
    "\n",
    "def get_articles(dom, date):\n",
    "    soup = BeautifulSoup(dom, 'html5lib')\n",
    "\n",
    "    paging_dev = soup.find('div', 'btn-group btn-group-paging')\n",
    "    prev_url = paging_dev.find_all('a')[1]['href']\n",
    "\n",
    "    articles = []\n",
    "    divs = soup.find_all('div', 'r-ent')\n",
    "    for div in divs:\n",
    "        if div.find('div', 'date').text.strip() == date:\n",
    "            push_count = 0\n",
    "            push_str = div.find('div', 'nrec').text\n",
    "            if push_str:\n",
    "                try:\n",
    "                    push_count = int(push_str)\n",
    "                except ValueError:\n",
    "                    if push_str == '爆':\n",
    "                        push_count = 99\n",
    "                    elif push_str.startswith('X'):\n",
    "                        push_count = -10\n",
    "\n",
    "            if div.find('a'):\n",
    "                href = div.find('a')['href']\n",
    "                title = div.find('a').text\n",
    "                author = div.find('div', 'author').text if div.find('div', 'author') else ''\n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'href': href,\n",
    "                    'push_count': push_count,\n",
    "                    'author': author\n",
    "                })\n",
    "    return articles, prev_url\n",
    "\n",
    "\n",
    "def parse(dom):\n",
    "    soup = BeautifulSoup(dom, 'html.parser')\n",
    "    links = soup.find(id='main-content').find_all('a')\n",
    "    img_urls = []\n",
    "    for link in links:\n",
    "        if re.match(r'^https?://(i.)?(m.)?imgur.com', link['href']):\n",
    "            img_urls.append(link['href'])\n",
    "    return img_urls\n",
    "\n",
    "\n",
    "def save(img_urls, title):\n",
    "    if img_urls:\n",
    "        try:\n",
    "            folder_name = title.strip()\n",
    "            os.makedirs(folder_name)\n",
    "            for img_url in img_urls:\n",
    "                # e.g. 'http://imgur.com/9487qqq.jpg'.split('//') -> ['http:', 'imgur.com/9487qqq.jpg']\n",
    "                if img_url.split('//')[1].startswith('m.'):\n",
    "                    img_url = img_url.replace('//m.', '//i.')\n",
    "                if not img_url.split('//')[1].startswith('i.'):\n",
    "                    img_url = img_url.split('//')[0] + '//i.' + img_url.split('//')[1]\n",
    "                if not img_url.endswith('.jpg'):\n",
    "                    img_url += '.jpg'\n",
    "                file_name = img_url.split('/')[-1]\n",
    "                urllib.request.urlretrieve(img_url, os.path.join(folder_name, file_name))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "def main():\n",
    "    current_page = get_web_content(PTT_URL + '/bbs/Beauty/index.html')\n",
    "    if current_page:\n",
    "        articles = []\n",
    "        date = time.strftime(\"%m/%d\").lstrip('0')\n",
    "        current_articles, prev_url = get_articles(current_page, date)\n",
    "        while current_articles:\n",
    "            articles += current_articles\n",
    "            current_page = get_web_content(PTT_URL + prev_url)\n",
    "            current_articles, prev_url = get_articles(current_page, date)\n",
    "\n",
    "        for article in articles:\n",
    "            print('Collecting beauty from:', article)\n",
    "            page = get_web_content(PTT_URL + article['href'])\n",
    "            if page:\n",
    "                img_urls = parse(page)\n",
    "                save(img_urls, article['title'])\n",
    "                article['num_image'] = len(img_urls)\n",
    "\n",
    "        with open('data.json', 'w', encoding='utf-8') as file:\n",
    "            json.dump(articles, file, indent=2, sort_keys=True, ensure_ascii=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取PTT當日的討論版發文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ch9_2(ptt_bbs_crawler.py)-爬取PTT當日的討論版發文\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 目標URL網址\n",
    "URL = \"https://www.ptt.cc\"\n",
    "TOPIC = \"BASEBALL\" #看板名稱選擇爬取Food板\n",
    "\n",
    "def get_resource(url):\n",
    "    headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "               \"AppleWebKit/537.36 (KHTML, like Gecko)\"\n",
    "               \"Chrome/63.0.3239.132 Safari/537.36\"}\n",
    "    return requests.get(url, headers=headers, cookies={\"over18\":\"1\"})\n",
    "\n",
    "def parse_html(r):\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        r.encoding = \"utf8\"\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")        \n",
    "    else:\n",
    "        print(\"HTTP請求錯誤...\" + url)\n",
    "        soup = None\n",
    "    \n",
    "    return soup    \n",
    "\n",
    "def get_articles(soup, date): #第一個參數取得今天所有發文，第二個參數是今天日期\n",
    "    articles = []  \n",
    "    # 取得上一頁的超連結\n",
    "    paging_div = soup.find(\"div\", class_=\"btn-group btn-group-paging\")\n",
    "    paging_a = paging_div.find_all(\"a\", class_=\"btn\")\n",
    "    prev_url = paging_a[1][\"href\"]\n",
    "\n",
    "    tag_divs = soup.find_all(\"div\", class_=\"r-ent\")\n",
    "    for tag in tag_divs:\n",
    "        # 判斷文章的日期\n",
    "        if tag.find(\"div\",class_=\"date\").text.strip() == date:\n",
    "            push_count = 0    # 取得推文數\n",
    "            push_str = tag.find(\"div\", class_=\"nrec\").text\n",
    "            if push_str:\n",
    "                try:\n",
    "                    push_count = int(push_str)  # 轉換成數字\n",
    "                except ValueError:  # 轉換失敗，可能是'爆'或 'X1','X2'\n",
    "                    if push_str == '爆':  #爆轉換成99\n",
    "                        push_count = 99\n",
    "                    elif push_str.startswith('X'):\n",
    "                        push_count = -10  #X開頭轉換成-10\n",
    "            # 取得發文的超連結和標題文字\n",
    "            if tag.find(\"a\"):  # 有超連結，表示文章存在\n",
    "                href = tag.find(\"a\")[\"href\"]\n",
    "                title = tag.find(\"a\").text\n",
    "                author = tag.find(\"div\", class_=\"author\").string \n",
    "                articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"href\": href,\n",
    "                    \"push_count\": push_count,\n",
    "                    \"author\": author\n",
    "                })\n",
    "    \n",
    "    return articles, prev_url\n",
    "\n",
    "def save_to_json(items, file):\n",
    "    with open(file, \"w\", encoding=\"utf-8\") as fp: # 寫入JSON檔案\n",
    "        json.dump(items,fp,indent=2,sort_keys=True,ensure_ascii=False)\n",
    "\n",
    "def web_scraping_bot(url):\n",
    "    articles = []\n",
    "    print(\"抓取網路資料中...\")\n",
    "    soup = parse_html(get_resource(url))\n",
    "    if soup:\n",
    "        # 取得今天日期, 去掉開頭'0'符合PTT的日期格式\n",
    "        today = time.strftime(\"%m/%d\").lstrip('0') \n",
    "        # 取得目前頁面的今日文章清單\n",
    "        current_articles, prev_url = get_articles(soup, today) \n",
    "        while current_articles: \n",
    "            articles += current_articles\n",
    "            print(\"等待2秒鐘...\")\n",
    "            time.sleep(2) \n",
    "             # 剖析上一頁繼續尋找是否有今日的文章\n",
    "            soup = parse_html(get_resource(URL + prev_url))\n",
    "            current_articles, prev_url = get_articles(soup, today)\n",
    "\n",
    "    return articles\n",
    "\n",
    "#建立指定看板的URL後，呼叫web_scraping_bot函數爬取發文資訊\n",
    "if __name__ == '__main__':\n",
    "    url = URL + \"/bbs/\" + TOPIC + \"/index.html\"\n",
    "    print(url)\n",
    "    articles = web_scraping_bot(url)\n",
    "    for item in articles:\n",
    "        print(item)\n",
    "    save_to_json(articles, \"articles.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八卦版鄉民從哪來?\n",
    "這隻爬蟲會去爬當前八卦版前50篇文章, 然後看這些發文的鄉民是來自哪個國家:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "取得今日文章列表:\n",
      "共 1402 篇文章\n",
      "取得前50篇文章的IP:\n",
      "查詢 IP: Re: [新聞] 飲恨身亡！李承翰父曾怨「速食店精神鑑定\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d17457537bf2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-d17457537bf2>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[0mip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_ip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m                 \u001b[0mcountry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_country\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcountry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcountry_to_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                     \u001b[0mcountry_to_count\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcountry\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d17457537bf2>\u001b[0m in \u001b[0;36mget_country\u001b[1;34m(ip)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_country\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mip\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFREEGEOIP_API\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mcountry_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'country_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'country_name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcountry_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc'\n",
    "FREEGEOIP_API = 'http://freegeoip.net/json/'\n",
    "\n",
    "\n",
    "def get_web_page(url):\n",
    "    resp = requests.get(url=url, cookies={'over18': '1'})\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url: ', resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        return resp.text\n",
    "\n",
    "\n",
    "def get_articles(dom, date):\n",
    "    soup = BeautifulSoup(dom, 'html5lib')\n",
    "    # Retrieve the link of previous page\n",
    "    paging_div = soup.find('div', 'btn-group btn-group-paging')\n",
    "    prev_url = paging_div.find_all('a')[1]['href']\n",
    "\n",
    "    articles = []\n",
    "    divs = soup.find_all('div', 'r-ent')\n",
    "    for d in divs:\n",
    "        # If post date matched:\n",
    "        if d.find('div', 'date').text.strip() == date:\n",
    "            # To retrieve the push count:\n",
    "            push_count = 0\n",
    "            push_str = d.find('div', 'nrec').text\n",
    "            if push_str:\n",
    "                try:\n",
    "                    push_count = int(push_str)\n",
    "                except ValueError:\n",
    "                    # If transform failed, it might be '爆', 'X1', 'X2', etc.\n",
    "                    if push_str == '爆':\n",
    "                        push_count = 99\n",
    "                    elif push_str.startswith('X'):\n",
    "                        push_count = -10\n",
    "\n",
    "            # To retrieve title and href of the article:\n",
    "            if d.find('a'):\n",
    "                href = d.find('a')['href']\n",
    "                title = d.find('a').text\n",
    "                author = d.find('div', 'author').text if d.find('div', 'author') else ''\n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'href': href,\n",
    "                    'push_count': push_count,\n",
    "                    'author': author\n",
    "                })\n",
    "\n",
    "    return articles, prev_url\n",
    "\n",
    "\n",
    "def get_ip(dom):\n",
    "    # e.g., ※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 27.52.6.175\n",
    "    pattern = '來自: \\d+\\.\\d+\\.\\d+\\.\\d+'\n",
    "    match = re.search(pattern, dom)\n",
    "    if match:\n",
    "        return match.group(0).replace('來自: ', '')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_country(ip):\n",
    "    if ip:\n",
    "        data = json.loads(requests.get(FREEGEOIP_API + ip).text)\n",
    "        country_name = data['country_name'] if data['country_name'] else None\n",
    "        return country_name\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('取得今日文章列表:')\n",
    "    current_page = get_web_page(PTT_URL + '/bbs/Gossiping/index.html')\n",
    "    if current_page:\n",
    "        articles = []\n",
    "        today = time.strftime('%m/%d').lstrip('0')\n",
    "        current_articles, prev_url = get_articles(current_page, today)\n",
    "        while current_articles:\n",
    "            articles += current_articles\n",
    "            current_page = get_web_page(PTT_URL + prev_url)\n",
    "            current_articles, prev_url = get_articles(current_page, today)\n",
    "        print('共 %d 篇文章' % (len(articles)))\n",
    "\n",
    "        print('取得前50篇文章的IP:')\n",
    "        country_to_count = dict()\n",
    "        for article in articles[:50]:\n",
    "            print('查詢 IP:', article['title'])\n",
    "            page = get_web_page(PTT_URL + article['href'])\n",
    "            if page:\n",
    "                ip = get_ip(page)\n",
    "                country = get_country(ip)\n",
    "                if country in country_to_count.keys():\n",
    "                    country_to_count[country] += 1\n",
    "                else:\n",
    "                    country_to_count[country] = 1\n",
    "\n",
    "        print('各國IP分佈: ')\n",
    "        for k, v in country_to_count.items():\n",
    "            print(k, v)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
