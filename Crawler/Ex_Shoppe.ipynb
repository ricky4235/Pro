{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ex19_Shoppe&狀態碼為200但無任何資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "使用偽裝user-agent爬取蝦皮購物網\n",
    "https://freelancerlife.info/zh/blog/python-web-scraping-user-agent-for-shopee/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "實戰中，雖然只要會使用最基本的requests和beautifulsoup的操作就可以爬許多網站，\n",
    "但那只是剛好對方的網站伺服器沒有任何檢查措施，就直接將網站的內容回應給你了。\n",
    "\n",
    "然而只要對方伺服器有任何檢查措施時，你就會得不到你想要的資訊，你可能被拒絕訪問，\n",
    "又或者是被重新導向到一個新的網頁。為了處理這種狀況，在使用requests套件時，\n",
    "可以額外加入user agent，使用虛擬的身份，這樣在拜訪網站時，對方的網頁伺服器如果認可你的身份，\n",
    "你就可以得到正確的網頁內容。\n",
    "\n",
    "以蝦皮購物作為範例，某些網站需要恰當的使用user-agent，才可以成功的獲取網頁內容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 當狀態碼為200，但伺服器卻不回傳給你任何商品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "當你一進入他的首頁時，在搜索欄中輸入你要查找的商品後，你就會跳到商品清單的頁面，請注意這裡的網址。\n",
    "\n",
    "\n",
    "我以PS4 pro 主機，進行搜索，你可以看到網址變成 https://shopee.tw/search?keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F 。\n",
    "\n",
    "很明顯這是以get的方式傳遞你輸入的變數(keyword)給對方的伺服器後台，之後進資料庫比對將最符合的商品傳出來，呈現給你。\n",
    "\n",
    "因此我們可以試試看手動改一下網址裡的keyword，然後直接用瀏覽器拜訪看看是不是能得到預期的東西。\n",
    "\n",
    "例如(XBOX one 主機): https://shopee.tw/search?keyword=XBOX%20one%20%E4%B8%BB%E6%A9%9F\n",
    "\n",
    "\n",
    "沒問題!你在瀏覽器中可以很簡單的直接改網址來獲得不一樣的商品，因此應該可以很簡單的設計一份網路爬蟲程式來抓取你要的商品。\n",
    "\n",
    "使用我們之前的requests、及beautifulsoup套件，你可以寫出例如以下的程式:\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://shopee.tw/search?keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F'\n",
    "r = requests.get(url)\n",
    "print(r.status_code)\n",
    "print(len(r.text))\n",
    "Copy\n",
    "200\n",
    "\n",
    "似乎沒有問題?狀態碼為200，代表沒有問題?\n",
    "\n",
    "接著試著使用beautifulesoup去剖析內容，在瀏覽器中檢查元素，可以得知商品是位在\n",
    "div class=\" col-xs-2-4 shopee-search-item-result__item\"底下，因此可以使用以下程式嘗試去剖析原始碼:\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "items = soup.find_all(\"div\", class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "print(len(items))\n",
    "Copy\n",
    "0\n",
    "\n",
    "沒有任何東西!!\n",
    "\n",
    "是的，你成功地拜訪了網站(status_code == 200)，但是伺服器卻不回傳給你任何商品。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 是不是被重新導向了<Response [302]>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "我們可以再檢查看看是不是被重新導向了，被連接到一個新的網址去。\n",
    "\n",
    "可以使用以下指令檢查:\n",
    "\n",
    "print(r.history)\n",
    "print(r.url)\n",
    "Copy\n",
    "[<Response [302]>]\n",
    "https://shopee.tw/search-item/?search=PS4%20pro%20%E4%B8%BB%E6%A9%9F\n",
    "\n",
    "302 可以簡單的理解為該資源原本確實存在，但已經被臨時改變了位置 。ref: https://zh.wikipedia.org/wiki/HTTP_302\n",
    "\n",
    "所以導致無法獲得商品資料。網址沒有改變，但是被臨時改變了位置?\n",
    "\n",
    "為了避免像這樣，status code顯示200，但事實上中途有被重新導向，而誤解為有正確的拜訪網站，\n",
    "我們可以在request.get中加入allow_redirects=False，\n",
    "\n",
    "如下:\n",
    "\n",
    "url = 'https://shopee.tw/search?keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F'\n",
    "r = requests.get(url,allow_redirects=False)\n",
    "print(r.status_code)\n",
    "Copy\n",
    "302\n",
    "\n",
    "直接出現302，現在我們可以避免中途明明被重新導向過，卻顯示一切正常。\n",
    "但是，我們依然無法獲得商品資訊，該怎麼做呢?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用user-agent，明確的告訴對方伺服器你是誰，然後如果認同你的身分的話就回傳給你正確的結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "這就是這篇教學的重點，使用user-agent，明確的告訴對方伺服器你是誰，然後如果認同你的身分的話就回傳給你正確的結果。\n",
    "\n",
    "我們可以使用fake_useragent隨機產生user-agent，放入requests.get裡面再試一次，\n",
    "\n",
    "請先安裝fake_useragent ref: https://pypi.org/project/fake-useragent/\n",
    "\n",
    "pip install fake-useragent\n",
    "Copy\n",
    "然後，試試看使用隨機的fake_useragent:\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "for i in range(5):\n",
    "    print(ua.random)\n",
    "    print('*-----------------*')\n",
    "Copy\n",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36\n",
    "*-----------------*\n",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.124 Safari/537.36\n",
    "*-----------------*\n",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; ru-RU) AppleWebKit/533.19.4 (KHTML, like Gecko) Version/5.0.3 Safari/533.19.4\n",
    "*-----------------*\n",
    "Mozilla/5.0 (Windows NT 5.1; rv:31.0) Gecko/20100101 Firefox/31.0\n",
    "*-----------------*\n",
    "Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36\n",
    "*-----------------*\n",
    "\n",
    "OK!\n",
    "\n",
    "接著以以下的方式將useragent放入headers在將放入request.get中，重新執行一次。\n",
    "\n",
    "如果你想當個很有禮貌的爬蟲，可以在額外加入你的email等聯絡方式。\n",
    "\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "url = 'https://shopee.tw/search?keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F'\n",
    "ua = UserAgent()\n",
    "headers = {\n",
    "    'User-Agent': ua.random,\n",
    "    'From': 'YOUR EMAIL ADDRESS'\n",
    "}\n",
    "\n",
    "r = requests.get(url,headers=headers,allow_redirects=False)\n",
    "print(r.status_code)\n",
    "print(r.history)\n",
    "print(r.url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "items = soup.find_all(\"div\", class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "print(len(items))\n",
    "Copy\n",
    "200\n",
    "[]\n",
    "https://shopee.tw/search-item/?search=PS4%20pro%20%E4%B8%BB%E6%A9%9F\n",
    "0\n",
    "\n",
    "這次成功的進入，並沒有被重新導向，但是依然沒有得到商品資訊。\n",
    "\n",
    "我猜測可能是如果沒有提供user-agent，對方伺服器會將你重新導向，而提供之後，雖然不會重新導向，\n",
    "但是user-agent並非是伺服器所認可的user-agent，就會拒絕將商品的資訊回傳給你，只丟給你一個空空的頁面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接看網站的robots.txt來猜測用哪個user-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "那麼要用哪個user-agent才有可能會成功呢?我們可以直接看他的robots.txt來進行猜測。\n",
    "\n",
    "https://shopee.tw/robots.txt\n",
    "\n",
    "User-Agent:Googlebot\n",
    "User-Agent:Bingbot\n",
    "Crawl-delay:0.1\n",
    "Disallow: /cart/\n",
    "Disallow: /checkout/\n",
    "Disallow: /buyer/\n",
    "Disallow: /user/\n",
    "Disallow: /me/\n",
    "Disallow: /order/\n",
    "Disallow: /daily_discover/\n",
    "Disallow: /mall/just-for-you/\n",
    "Disallow: /mall/*-cat.\n",
    "Disallow: /from_same_shop/\n",
    "Disallow: /you_may_also_like/\n",
    "Disallow: *-i.*/similar?from=flash_sale\n",
    "Disallow: /find_similar_products/\n",
    "Disallow: /top_products\n",
    "Disallow: /search*searchPrefill\n",
    "\n",
    "User-Agent:*\n",
    "Crawl-delay:1\n",
    "Disallow: /cart/\n",
    "Disallow: /checkout/\n",
    "Disallow: /buyer/\n",
    "Disallow: /user/\n",
    "Disallow: /me/\n",
    "Disallow: /order/\n",
    "Disallow: /daily_discover/\n",
    "Disallow: /mall/just-for-you/\n",
    "Disallow: /mall/*-cat.\n",
    "Disallow: /from_same_shop/\n",
    "Disallow: /you_may_also_like/\n",
    "Disallow: *-i.*/similar\n",
    "Disallow: /find_similar_products/\n",
    "Disallow: /top_products\n",
    "Disallow: /search*searchPrefill\n",
    "Copy\n",
    "既然robots.txt中第一個提到的user-agent就是Googlebot，何不以Googlebot來試試呢?\n",
    "\n",
    "url = 'https://shopee.tw/search?keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F'\n",
    "headers = {\n",
    "    'User-Agent': 'Googlebot',\n",
    "    'From': 'YOUR EMAIL ADDRESS'\n",
    "}\n",
    "\n",
    "r = requests.get(url,headers=headers,allow_redirects=True)\n",
    "print(r.status_code)\n",
    "print(r.history)\n",
    "print(r.url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "items = soup.find_all(\"div\", class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "print(len(items))\n",
    "Copy\n",
    "200\n",
    "[]\n",
    "https://shopee.tw/search?keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F\n",
    "50\n",
    "\n",
    "成功!! 有50個商品!!\n",
    "\n",
    "使用了Googlebot，對方伺服器認可了你所偽裝的身分，正確的將商品資訊傳給了你。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "這次我們試圖以requests拜訪蝦皮購物網，經過多方嘗試，終於在設定user-agent等於Googlebot的情況下，成功地獲得了商品資訊。\n",
    "筆者也使用過robots.txt裡提到的Bingbot，但是也無法獲得商品資訊，只能說是湊巧成功了。\n",
    "是不是非常驚險刺激呢?爬蟲程式最有趣的地方就在於猜測對方伺服器後台是如何傳送資料，然後設法以各種手段獲得內容。\n",
    "\n",
    "另外，仔細檢查蝦皮購物網的內容後，事實上對方是有api可以快速呼叫商品內容的，\n",
    "如: https://shopee.tw/api/v2/search_items/?by=relevancy&keyword=PS4%20pro%20%E4%B8%BB%E6%A9%9F&limit=20&newest=0&order=desc&page_type=search\n",
    "，會得到包含商品資訊的json檔，然而一樣在python底下使用requests試圖爬取時，要用Googlebot才有辦法獲得該資訊。\n",
    "\n",
    "當我們能夠成功地爬取購物網的商品內容後，就有很多可能的應用，例如:\n",
    "\n",
    "(1) 編寫多個不同電商網站的爬蟲程式，放在一起，變成一個比價程式，讓使用者只要輸入商品名稱，\n",
    "就能跳出各大電商網站該商品的價格，讓使用者可以找到哪個購物網有比較便宜的價格。\n",
    "\n",
    "(2) 讓使用者能夠快速的搜尋多項商品並且比較，例如，可以快速地比較PS4和XBox的價格大約為在哪裡。\n",
    "\n",
    "(3) 可以將電商網站的爬蟲程式寫入排程器裡面(如linux底下的crontab)每隔一段時間執行一次，\n",
    "將商品價格儲存下來，如此，經過一段時間，我們就可以進行商品價格波動的分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本章節程式碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def shopee_scraper(keyword,n_page=0,used=False,new=False):\n",
    "    '''\n",
    "    參數說明:\n",
    "        keyword: 商品名稱關鍵字\n",
    "        n_page: 第幾頁(每頁有50個商品)\n",
    "        used: 是否為二手商品?\n",
    "        new: 是否為新商品?\n",
    "    '''\n",
    "    url = f'https://shopee.tw/search?keyword={keyword}&page={n_page}&sortBy=relevancy'\n",
    "    if used:\n",
    "        url += '&newItem=true'\n",
    "    if new:\n",
    "        url += '&usedItem=true'\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Googlebot',\n",
    "        'From': 'YOUR EMAIL ADDRESS'\n",
    "    }\n",
    "    \n",
    "    r = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    contents = soup.find_all(\"div\", class_=\"_1NoI8_ _2gr36I\")\n",
    "    prices = soup.find_all(\"span\", class_=\"_341bF0\")\n",
    "    all_items = soup.find_all(\"div\", class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "    links = [i.find('a').get('href') for i in all_items]\n",
    "    \n",
    "    for c, p, l in zip(contents, prices, links):\n",
    "        print(c.contents[0])\n",
    "        print(p.contents[0])\n",
    "        print('https://shopee.tw/'+l)\n",
    "        print('*---------------------------------*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 各國蝦皮網頁格式都一樣，以下Code只要改一下網址就可以爬不同國家"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取【菲律賓蝦皮】搜尋頁面簡易商品資料_20200610完整版\n",
    "1. 使用針對蝦皮的headers = {\"user-agent\": \"Googlebot\"}取得解析網頁資料\n",
    "2. 先取得搜尋頁數的網址List\n",
    "3. 直接爬取網址List之商品資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "def get_urls(url, query, start_page, end_page): \n",
    "    urls = []\n",
    "    for page in range(start_page, end_page+1):\n",
    "        urls.append(url.format(query, page-1))    #query帶入url的{0}、page帶入{1}\n",
    "    return urls\n",
    "\n",
    "def get_resource(url):\n",
    "    ua = UserAgent()\n",
    "    headers = {\"user-agent\": \"Googlebot\"}\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "def parse_html(html_str):\n",
    "    return BeautifulSoup(html_str, \"lxml\")\n",
    "\n",
    "def get_goods(soup):\n",
    "    goods = []\n",
    "    rows = soup.find_all(\"div\",class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "\n",
    "    for row in rows:\n",
    "\n",
    "        try:\n",
    "            name = row.find(\"div\", class_=\"_1NoI8_ _16BAGk\").text\n",
    "        except:\n",
    "            name = None\n",
    "\n",
    "        try:\n",
    "            price = row.find(\"span\", class_=\"_341bF0\").text\n",
    "        except:\n",
    "            price = None\n",
    "            \n",
    "        try:\n",
    "            Original_price = row.find(\"div\", class_=\"_1w9jLI QbH7Ig U90Nhh\").text\n",
    "        except:\n",
    "            Original_price = None\n",
    "\n",
    "        try:\n",
    "            sold = row.find(\"div\", class_=\"_18SLBt\").text\n",
    "        except:\n",
    "            sold = None\n",
    "\n",
    "        try:\n",
    "            link = \"https://ph.xiapibuy.com/\" + row.find(\"a\").get('href')\n",
    "        except:\n",
    "            link = None\n",
    "        \n",
    "        good= [name, price, Original_price, sold, link]\n",
    "        goods.append(good)\n",
    "    return goods\n",
    "\n",
    "def web_scraping_bot(urls):\n",
    "    all_goods = [[\"品名\",\"價格(單位:千)\",\"原價(單位:千)\",\"售出量\", \"網址\"]]  #巢狀清單\n",
    "    page = 1\n",
    "    \n",
    "    for url in urls:\n",
    "        print(\"抓取: 第\" + str(page) + \"頁 網路資料中...\")\n",
    "        page = page + 1\n",
    "        r = get_resource(url)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            soup = parse_html(r.text)\n",
    "            goods = get_goods(soup)\n",
    "            all_goods = all_goods + goods\n",
    "            print(\"等待5秒鐘...\")\n",
    "            \n",
    "            #當目前頁數=所有頁數時，跳出迴圈\n",
    "            now_page = soup.find(\"span\", class_=\"shopee-mini-page-controller__current\").text\n",
    "            all_page = soup.find(\"span\", class_=\"shopee-mini-page-controller__total\").text\n",
    "            if now_page == all_page:\n",
    "                break   #已經沒有下一頁\n",
    "            time.sleep(5) \n",
    "        else:\n",
    "            print(\"HTTP請求錯誤...\")\n",
    "\n",
    "    return all_goods\n",
    "\n",
    "def save_to_csv(items, file):\n",
    "    with open(file, \"w+\", newline=\"\", encoding=\"utf_8_sig\") as fp:  #utf_8_sig:能讓輸出的csv正確顯示中文(utf_8會有亂碼)\n",
    "        writer = csv.writer(fp)\n",
    "        for item in items:\n",
    "            writer.writerow(item)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 目標URL網址\n",
    "    \"\"\"直接在蝦皮搜尋\"\"\"\n",
    "    #url = \"https://ph.xiapibuy.com/search?keyword={0}&page={1}\"\n",
    "    \"\"\"在電腦配件中搜尋\"\"\"\n",
    "    url = \"https://ph.xiapibuy.com/search?category=18596&keyword={0}&subcategory=18613&page={1}\"\n",
    "    \"\"\"在電腦遊戲中搜尋\"\"\"\n",
    "    #url = \"https://ph.xiapibuy.com/search?category=20718&keyword={0}&subcategory=20729&page={1}\"\n",
    "\n",
    "    print(get_resource(url).status_code)\n",
    "\n",
    "    urls = get_urls(url, \"genius\", 1, 3)\n",
    "    print(urls)\n",
    "\n",
    "    goods = web_scraping_bot(urls)\n",
    "    df = pd.DataFrame(goods)       #用dataframe列出\n",
    "    print(df)\n",
    "    \n",
    "    save_to_csv(goods, \"x.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬取【印尼蝦皮】搜尋頁面簡易商品資料_20200610完整版\n",
    "1. 使用針對蝦皮的headers = {\"user-agent\": \"Googlebot\"}取得解析網頁資料\n",
    "2. 先取得搜尋頁數的網址List\n",
    "3. 直接爬取網址List之商品資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "def get_urls(url, query, start_page, end_page): \n",
    "    urls = []\n",
    "    for page in range(start_page, end_page+1):\n",
    "        urls.append(url.format(query, page-1))    #query帶入url的{0}、page帶入{1}\n",
    "    return urls\n",
    "\n",
    "def get_resource(url):\n",
    "    ua = UserAgent()\n",
    "    headers = {\"user-agent\": \"Googlebot\"}\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "def parse_html(html_str):\n",
    "    return BeautifulSoup(html_str, \"lxml\")\n",
    "\n",
    "def get_goods(soup):\n",
    "    goods = []\n",
    "    rows = soup.find_all(\"div\",class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "\n",
    "    for row in rows:\n",
    "\n",
    "        try:\n",
    "            name = row.find(\"div\", class_=\"_1NoI8_ _16BAGk\").text\n",
    "        except:\n",
    "            name = None\n",
    "\n",
    "        try:\n",
    "            price = row.find(\"span\", class_=\"_341bF0\").text\n",
    "        except:\n",
    "            price = None\n",
    "            \n",
    "        try:\n",
    "            Original_price = row.find(\"div\", class_=\"_1w9jLI QbH7Ig U90Nhh\").text.replace(\"Rp\",\"\")\n",
    "        except:\n",
    "            Original_price = None\n",
    "\n",
    "        try:\n",
    "            sold = row.find(\"div\", class_=\"_18SLBt\").text.replace(\" Terjual\",\"\")\n",
    "            #假如字串有包含RB，將RB取代掉，並將字串轉為浮點數，再乘以1000\n",
    "            if \"RB\" in sold:\n",
    "                sold = sold.replace(\"RB\",\"\")\n",
    "                sold = float(sold)*1000\n",
    "        except:\n",
    "            sold = None\n",
    "\n",
    "        try:\n",
    "            link = \"https://id.xiapibuy.com/\" + row.find(\"a\").get('href')\n",
    "        except:\n",
    "            link = None\n",
    "        \n",
    "        good= [name, price, Original_price, sold, link]\n",
    "        goods.append(good)\n",
    "    return goods\n",
    "\n",
    "def web_scraping_bot(urls):\n",
    "    all_goods = [[\"品名\",\"價格(單位:千)\",\"原價(單位:千)\",\"售出量\", \"網址\"]]  #巢狀清單\n",
    "    page = 1\n",
    "    \n",
    "    for url in urls:\n",
    "        print(\"抓取: 第\" + str(page) + \"頁 網路資料中...\")\n",
    "        page = page + 1\n",
    "        r = get_resource(url)\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            soup = parse_html(r.text)\n",
    "            goods = get_goods(soup)\n",
    "            all_goods = all_goods + goods\n",
    "            print(\"等待5秒鐘...\")\n",
    "            \n",
    "            #當目前頁數=所有頁數時，跳出迴圈\n",
    "            now_page = soup.find(\"span\", class_=\"shopee-mini-page-controller__current\").text\n",
    "            all_page = soup.find(\"span\", class_=\"shopee-mini-page-controller__total\").text\n",
    "            if now_page == all_page:\n",
    "                break   #已經沒有下一頁\n",
    "            time.sleep(5) \n",
    "        else:\n",
    "            print(\"HTTP請求錯誤...\")\n",
    "\n",
    "    return all_goods\n",
    "\n",
    "def save_to_csv(items, file):\n",
    "    with open(file, \"w+\", newline=\"\", encoding=\"utf_8_sig\") as fp:  #utf_8_sig:能讓輸出的csv正確顯示中文(utf_8會有亂碼)\n",
    "        writer = csv.writer(fp)\n",
    "        for item in items:\n",
    "            writer.writerow(item)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 目標URL網址\n",
    "    \"\"\"直接在蝦皮搜尋\"\"\"\n",
    "    #url = \"https://id.xiapibuy.com/search?keyword={0}&page={1}\"\n",
    "    \"\"\"在電腦配件中搜尋\"\"\"\n",
    "    #url = \"https://id.xiapibuy.com/search?category=134&keyword={0}&page={1}\"\n",
    "    \"\"\"直接搜尋品牌\"\"\"\n",
    "    url = \"https://id.xiapibuy.com/search?attrId=14478&attrName=Merek&attrVal={0}&page={1}\"\n",
    "\n",
    "    print(get_resource(url).status_code)\n",
    "\n",
    "    urls = get_urls(url, \"genius\", 1, 3)\n",
    "    print(urls)\n",
    "\n",
    "    goods = web_scraping_bot(urls)\n",
    "    df = pd.DataFrame(goods)       #用dataframe列出\n",
    "    print(df)\n",
    "    \n",
    "    save_to_csv(goods, \"x.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遍歷【印尼蝦皮】各商品爬取細節_20200609完整版\n",
    "1. 使用針對蝦皮的headers = {\"user-agent\": \"Googlebot\"}取得解析網頁資料\n",
    "2. 先取得搜尋頁數的網址List\n",
    "3. 進入每個商品頁面\n",
    "4. 爬取進入頁面之商品細節"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://id.xiapibuy.com/search?category=134&keyword=genius&page=0&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=1&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=2&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=3&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=4&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=5&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=6&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=7&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=8&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=9&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=10&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=11&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=12&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=13&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=14&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=15&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=16&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=17&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=18&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=19&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=20&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=21&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=22&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=23&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=24&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=25&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=26&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=27&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=28&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=29&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=30&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=31&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=32&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=33&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=34&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=35&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=36&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=37&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=38&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=39&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=40&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=41&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=42&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=43&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=44&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=45&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=46&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=47&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=48&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=49&sortBy=sales', 'https://id.xiapibuy.com/search?category=134&keyword=genius&page=50&sortBy=sales']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2a9ed3cf8d53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"genius\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscraping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m     \u001b[0msave_to_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"m.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-2a9ed3cf8d53>\u001b[0m in \u001b[0;36mscraping\u001b[1;34m(urls)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mscraping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0mall_goods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"price\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Original_price\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"star\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"reviews\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"sold\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"stock\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"seller\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"seller_link\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"seller_from\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"category\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"brand\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"description\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"URL\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFindLinks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m#記錄目前進行的迴圈次數，配上總迴圈次數，可做為進度條使用。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Crawing No.\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Item in Total:\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFindLinks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"Item\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-2a9ed3cf8d53>\u001b[0m in \u001b[0;36mFindLinks\u001b[1;34m(pages)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mlinklist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"col-xs-2-4 shopee-search-item-result__item\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlinks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-2a9ed3cf8d53>\u001b[0m in \u001b[0;36mget_soup\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_soup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"user-agent\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Googlebot\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Unexpected EOF'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1811\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1813\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1814\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re, time, requests, csv\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "#解析(蝦皮headers要用Googlebot)\n",
    "def get_soup(url):\n",
    "    headers = {\"user-agent\": \"Googlebot\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    return BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "#搜尋網址&換頁\n",
    "def get_urls(url, query, start_page, end_page): \n",
    "    urls = []\n",
    "    \n",
    "    for page in range(start_page, end_page+1):\n",
    "        urls.append(url.format(query, page))    #query帶入url的{0}、page帶入{1}\n",
    "    print(urls)    \n",
    "    return urls\n",
    "\n",
    "# 依序爬取每頁點入網址\n",
    "def FindLinks(pages):\n",
    "    linklist = []\n",
    "    for page in pages:  \n",
    "        soup = get_soup(page)\n",
    "        links = soup.find_all(\"div\",class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "        for link in links:\n",
    "            k = \"https://id.xiapibuy.com/\" + link.find(\"a\").get('href')\n",
    "            linklist.append(k)\n",
    "    return linklist\n",
    "\n",
    "# 爬取點入分頁資料\n",
    "def get_goods(url):\n",
    "    goods = []\n",
    "    rows = get_soup(url)\n",
    "\n",
    "    for row in rows:\n",
    "\n",
    "        try:\n",
    "            name = row.find('div', class_=\"qaNIZv\").text\n",
    "        except:\n",
    "            name = None\n",
    "\n",
    "        try:\n",
    "            #price = row.find('div', class_=\"_3n5NQx\").text.replace(\"Rp\",\"\")\n",
    "            price = row.select_one(\"._3n5NQx\").get_text().replace(\"Rp\",\"\")\n",
    "        except:\n",
    "            price = None\n",
    "            \n",
    "        try:\n",
    "            Original_price = row.find(\"div\", class_=\"_3_ISdg\").text.replace(\"Rp\",\"\")\n",
    "        except:\n",
    "            Original_price = None\n",
    "            \n",
    "        try:\n",
    "            star = row.find('div', '_3Oj5_n _2z6cUg').text\n",
    "        except:\n",
    "            star = None\n",
    "\n",
    "        try:\n",
    "            #reviews = row.select(\"._3Oj5_n\")[1].get_text()\n",
    "            reviews = row.find_all('div', '_3Oj5_n')[1].text\n",
    "        except:\n",
    "            reviews = None\n",
    "            \n",
    "        try:\n",
    "            sold = row.find('div', '_22sp0A').text\n",
    "        except:\n",
    "            sold = None\n",
    "            \n",
    "        try:\n",
    "            #stock = row.select(\"._1FzU2Y .items-center div+ div\")[0].get_text()\n",
    "            stock = row.find(\"div\", \"flex items-center _1FzU2Y\").get_text()\n",
    "        except:\n",
    "            stock = None\n",
    "        \n",
    "        try:\n",
    "            seller = row.select_one(\"._3Lybjn\").get_text()\n",
    "        except:\n",
    "            seller = None\n",
    "            \n",
    "        try:\n",
    "            seller_link = \"https://id.xiapibuy.com\" + row.find(\"a\", \"btn btn-light btn--s btn--inline btn-light--link Ed2lAD\").get('href')\n",
    "        except:\n",
    "            seller_link = None\n",
    "            \n",
    "        try:\n",
    "            seller_from = row.select(\".kIo6pj\")[-1].div.get_text()\n",
    "            #seller_from = row.find_all(\"kIo6pj\")[-1].text\n",
    "        except:\n",
    "            seller_from = None\n",
    "            \n",
    "        try:\n",
    "            category = row.select_one(\".kIo6pj\").get_text().replace(\"Kategori\", \"\")\n",
    "        except:\n",
    "            category = None\n",
    "\n",
    "        try:\n",
    "            brand = row.select_one(\"._2H-513\").get_text()\n",
    "        except:\n",
    "            brand = None\n",
    "\n",
    "        try:\n",
    "            description = row.find(\"div\", \"_2u0jt9\").get_text()\n",
    "        except:\n",
    "            description = None\n",
    "            \n",
    "        try:\n",
    "            URL = url\n",
    "        except:\n",
    "            URL = None\n",
    "            \n",
    "        good= [name, price, Original_price, star, reviews, sold, stock, seller, seller_link, seller_from, category, brand, description, URL]\n",
    "        goods.append(good)\n",
    "        \n",
    "    return goods[1]  #因為不知為何第[0]列都會出現一排None，只好取第[1]列\n",
    "\n",
    "# 將每一個點入頁面的List依序爬取\n",
    "def scraping(urls):\n",
    "    all_goods = [[\"name\",\"price\",\"Original_price\",\"star\",\"reviews\",\"sold\",\"stock\",\"seller\",\"seller_link\",\"seller_from\",\"category\",\"brand\",\"description\", \"URL\"]]\n",
    "    for idx,i in enumerate(FindLinks(urls)):  #記錄目前進行的迴圈次數，配上總迴圈次數，可做為進度條使用。\n",
    "        print(\"Crawing No.\" + str(idx+1) + \" Item in Total:\" + str(len(FindLinks(urls))) + \"Item\")\n",
    "        \n",
    "        goods = get_goods(i)\n",
    "        time.sleep(0.2)\n",
    "        all_goods.append(goods)\n",
    "    return all_goods\n",
    "\n",
    "#存成CSV\n",
    "def save_to_csv(items, file):\n",
    "    with open(file, \"w+\", newline=\"\", encoding=\"utf_8_sig\") as fp:  #utf_8_sig:能讓輸出的csv正確顯示中文(utf_8會有亂碼)\n",
    "        writer = csv.writer(fp)\n",
    "        for item in items:\n",
    "            writer.writerow(item)\n",
    "    \n",
    "# 開始爬蟲\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"直接在蝦皮搜尋\"\"\"\n",
    "    #url = \"https://id.xiapibuy.com/search?keyword={0}&page={1}\"\n",
    "    \"\"\"在電腦配件中搜尋\"\"\"\n",
    "    #url = \"https://id.xiapibuy.com/search?category=134&keyword={0}&page={1}\"\n",
    "    \"\"\"直接搜尋品牌\"\"\"\n",
    "    #url = \"https://id.xiapibuy.com/search?attrId=14478&attrName=Merek&attrVal={0}&page={1}\"\n",
    "    \"\"\"在電腦配件中搜尋+照暢銷排序\"\"\"\n",
    "    url = \"https://id.xiapibuy.com/search?category=134&keyword={0}&page={1}&sortBy=sales\"\n",
    "    \n",
    "    urls = get_urls(url, \"genius\", 0, 50)\n",
    "    \n",
    "    m = scraping(urls)\n",
    "    save_to_csv(m, \"m.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遍歷【台灣蝦皮】各商品爬取細節_20200610完整版\n",
    "1. 先取得搜尋頁數的網址List\n",
    "2. 進入每個商品頁面\n",
    "3. 爬取進入頁面之商品細節"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time, requests, csv\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "#解析(蝦皮headers要用Googlebot)\n",
    "def get_soup(url):\n",
    "    headers = {\"user-agent\": \"Googlebot\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    return BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "#搜尋網址&換頁\n",
    "def get_urls(url, query, start_page, end_page): \n",
    "    urls = []\n",
    "    \n",
    "    for page in range(start_page, end_page+1):\n",
    "        urls.append(url.format(query, page))    #query帶入url的{0}、page帶入{1}\n",
    "    print(urls)    \n",
    "    return urls\n",
    "\n",
    "# 依序爬取每頁點入網址\n",
    "def FindLinks(pages):\n",
    "    linklist = []\n",
    "    for page in pages:  \n",
    "        soup = get_soup(page)\n",
    "        links = soup.find_all(\"div\",class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "        for link in links:\n",
    "            k = \"https://shopee.tw/\" + link.find(\"a\").get('href')\n",
    "            linklist.append(k)\n",
    "    return linklist\n",
    "\n",
    "# 爬取點入分頁資料\n",
    "def get_goods(url):\n",
    "    goods = []\n",
    "    rows = get_soup(url)\n",
    "\n",
    "    for row in rows:\n",
    "\n",
    "        try:\n",
    "            name = row.find('div', class_=\"qaNIZv\").text\n",
    "        except:\n",
    "            name = None\n",
    "\n",
    "        try:\n",
    "            #price = row.find('div', class_=\"_3n5NQx\").text\n",
    "            price = row.select_one(\"._3n5NQx\").get_text()\n",
    "        except:\n",
    "            price = None\n",
    "            \n",
    "        try:\n",
    "            Original_price = row.find(\"div\", class_=\"_3_ISdg\").text\n",
    "        except:\n",
    "            Original_price = None\n",
    "            \n",
    "        try:\n",
    "            star = row.find('div', '_3Oj5_n _2z6cUg').text\n",
    "        except:\n",
    "            star = None\n",
    "\n",
    "        try:\n",
    "            #reviews = row.select(\"._3Oj5_n\")[1].get_text()\n",
    "            reviews = row.find_all('div', '_3Oj5_n')[1].text\n",
    "        except:\n",
    "            reviews = None\n",
    "            \n",
    "        try:\n",
    "            sold = row.find('div', '_22sp0A').text\n",
    "        except:\n",
    "            sold = None\n",
    "            \n",
    "        try:\n",
    "            #stock = row.select(\"._1FzU2Y .items-center div+ div\")[0].get_text()\n",
    "            stock = row.find(\"div\", \"flex items-center _1FzU2Y\").get_text()\n",
    "        except:\n",
    "            stock = None\n",
    "        \n",
    "        try:\n",
    "            seller = row.select_one(\"._3Lybjn\").get_text()\n",
    "        except:\n",
    "            seller = None\n",
    "            \n",
    "        try:\n",
    "            seller_link = \"https://shopee.tw\" + row.find(\"a\", \"btn btn-light btn--s btn--inline btn-light--link Ed2lAD\").get('href')\n",
    "        except:\n",
    "            seller_link = None\n",
    "            \n",
    "        try:\n",
    "            seller_from = row.select(\".kIo6pj\")[-1].div.get_text()\n",
    "            #seller_from = row.find_all(\"kIo6pj\")[-1].text\n",
    "        except:\n",
    "            seller_from = None\n",
    "            \n",
    "        try:\n",
    "            category = row.select_one(\".kIo6pj\").get_text().replace(\"Kategori\", \"\")\n",
    "        except:\n",
    "            category = None\n",
    "\n",
    "        try:\n",
    "            brand = row.select_one(\"._2H-513\").get_text()\n",
    "        except:\n",
    "            brand = None\n",
    "\n",
    "        try:\n",
    "            description = row.find(\"div\", \"_2u0jt9\").get_text()\n",
    "        except:\n",
    "            description = None\n",
    "            \n",
    "        try:\n",
    "            URL = url\n",
    "        except:\n",
    "            URL = None\n",
    "            \n",
    "        good= [name, price, Original_price, star, reviews, sold, stock, seller, seller_link, seller_from, category, brand, description, URL]\n",
    "        goods.append(good)\n",
    "        \n",
    "    return goods[1]  #因為不知為何第[0]列都會出現一排None，只好取第[1]列\n",
    "\n",
    "# 將每一個點入頁面的List依序爬取\n",
    "def scraping(urls):\n",
    "    all_goods = [[\"name\",\"price\",\"Original_price\",\"star\",\"reviews\",\"sold\",\"stock\",\"seller\",\"seller_link\",\"seller_from\",\"category\",\"brand\",\"description\", \"URL\"]]\n",
    "    for idx,i in enumerate(FindLinks(urls)):  #記錄目前進行的迴圈次數，配上總迴圈次數，可做為進度條使用。\n",
    "        print(\"Crawing No.\" + str(idx+1) + \" Item in Total:\" + str(len(FindLinks(urls))) + \"Item\")\n",
    "        \n",
    "        goods = get_goods(i)\n",
    "        time.sleep(0.2)\n",
    "        all_goods.append(goods)\n",
    "    return all_goods\n",
    "\n",
    "#存成CSV\n",
    "def save_to_csv(items, file):\n",
    "    with open(file, \"w+\", newline=\"\", encoding=\"utf_8_sig\") as fp:  #utf_8_sig:能讓輸出的csv正確顯示中文(utf_8會有亂碼)\n",
    "        writer = csv.writer(fp)\n",
    "        for item in items:\n",
    "            writer.writerow(item)\n",
    "    \n",
    "# 開始爬蟲\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"直接在蝦皮搜尋\"\"\"\n",
    "    url = \"https://shopee.tw/search?keyword={0}&page={1}\"\n",
    "    \"\"\"在電腦配件中搜尋\"\"\"\n",
    "    #url = \"https://shopee.tw/search?category=134&keyword={0}&page={1}\"\n",
    "    \"\"\"直接搜尋品牌\"\"\"\n",
    "    #url = \"https://shopee.tw/search?attrId=14478&attrName=Merek&attrVal={0}&page={1}\"\n",
    "    \n",
    "    urls = get_urls(url, \"昆盈\", 0, 1)\n",
    "    \n",
    "    m = scraping(urls)\n",
    "    save_to_csv(m, \"m.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "菲律賓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time, requests, csv\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "#解析(蝦皮headers要用Googlebot)\n",
    "def get_soup(url):\n",
    "    headers = {\"user-agent\": \"Googlebot\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    return BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "#搜尋網址&換頁\n",
    "def get_urls(url, query, start_page, end_page): \n",
    "    urls = []\n",
    "    \n",
    "    for page in range(start_page, end_page+1):\n",
    "        urls.append(url.format(query, page))    #query帶入url的{0}、page帶入{1}\n",
    "    print(urls)    \n",
    "    return urls\n",
    "\n",
    "# 依序爬取每頁點入網址\n",
    "def FindLinks(pages):\n",
    "    linklist = []\n",
    "    for page in pages:  \n",
    "        soup = get_soup(page)\n",
    "        links = soup.find_all(\"div\",class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "        for link in links:\n",
    "            k = \"https://ph.xiapibuy.com/\" + link.find(\"a\").get('href')\n",
    "            linklist.append(k)\n",
    "    return linklist\n",
    "\n",
    "# 爬取點入分頁資料\n",
    "def get_goods(url):\n",
    "    goods = []\n",
    "    rows = get_soup(url)\n",
    "\n",
    "    for row in rows:\n",
    "\n",
    "        try:\n",
    "            name = row.find('div', class_=\"qaNIZv\").text\n",
    "        except:\n",
    "            name = None\n",
    "\n",
    "        try:\n",
    "            #price = row.find('div', class_=\"_3n5NQx\").text.replace(\"Rp\",\"\")\n",
    "            price = row.select_one(\"._3n5NQx\").get_text().replace(\"Rp\",\"\")\n",
    "        except:\n",
    "            price = None\n",
    "            \n",
    "        try:\n",
    "            Original_price = row.find(\"div\", class_=\"_3_ISdg\").text.replace(\"Rp\",\"\")\n",
    "        except:\n",
    "            Original_price = None\n",
    "            \n",
    "        try:\n",
    "            star = row.find('div', '_3Oj5_n _2z6cUg').text\n",
    "        except:\n",
    "            star = None\n",
    "\n",
    "        try:\n",
    "            #reviews = row.select(\"._3Oj5_n\")[1].get_text()\n",
    "            reviews = row.find_all('div', '_3Oj5_n')[1].text\n",
    "        except:\n",
    "            reviews = None\n",
    "            \n",
    "        try:\n",
    "            sold = row.find('div', '_22sp0A').text\n",
    "        except:\n",
    "            sold = None\n",
    "            \n",
    "        try:\n",
    "            #stock = row.select(\"._1FzU2Y .items-center div+ div\")[0].get_text()\n",
    "            stock = row.find(\"div\", \"flex items-center _1FzU2Y\").get_text()\n",
    "        except:\n",
    "            stock = None\n",
    "        \n",
    "        try:\n",
    "            seller = row.select_one(\"._3Lybjn\").get_text()\n",
    "        except:\n",
    "            seller = None\n",
    "            \n",
    "        try:\n",
    "            seller_link = \"https://ph.xiapibuy.com\" + row.find(\"a\", \"btn btn-light btn--s btn--inline btn-light--link Ed2lAD\").get('href')\n",
    "        except:\n",
    "            seller_link = None\n",
    "            \n",
    "        try:\n",
    "            seller_from = row.select(\".kIo6pj\")[-1].div.get_text()\n",
    "            #seller_from = row.find_all(\"kIo6pj\")[-1].text\n",
    "        except:\n",
    "            seller_from = None\n",
    "            \n",
    "        try:\n",
    "            category = row.select_one(\".kIo6pj\").get_text().replace(\"Kategori\", \"\")\n",
    "        except:\n",
    "            category = None\n",
    "\n",
    "        try:\n",
    "            brand = row.select_one(\"._2H-513\").get_text()\n",
    "        except:\n",
    "            brand = None\n",
    "\n",
    "        try:\n",
    "            description = row.find(\"div\", \"_2u0jt9\").get_text()\n",
    "        except:\n",
    "            description = None\n",
    "            \n",
    "        try:\n",
    "            URL = url\n",
    "        except:\n",
    "            URL = None\n",
    "            \n",
    "        good= [name, price, Original_price, star, reviews, sold, stock, seller, seller_link, seller_from, category, brand, description, URL]\n",
    "        goods.append(good)\n",
    "        \n",
    "    return goods[1]  #因為不知為何第[0]列都會出現一排None，只好取第[1]列\n",
    "\n",
    "# 將每一個點入頁面的List依序爬取\n",
    "def scraping(urls):\n",
    "    all_goods = [[\"name\",\"price\",\"Original_price\",\"star\",\"reviews\",\"sold\",\"stock\",\"seller\",\"seller_link\",\"seller_from\",\"category\",\"brand\",\"description\", \"URL\"]]\n",
    "    for idx,i in enumerate(FindLinks(urls)):  #記錄目前進行的迴圈次數，配上總迴圈次數，可做為進度條使用。\n",
    "        print(\"Crawing No.\" + str(idx+1) + \" Item in Total:\" + str(len(FindLinks(urls))) + \"Item\")\n",
    "        \n",
    "        goods = get_goods(i)\n",
    "        time.sleep(0.2)\n",
    "        all_goods.append(goods)\n",
    "    return all_goods\n",
    "\n",
    "#存成CSV\n",
    "def save_to_csv(items, file):\n",
    "    with open(file, \"w+\", newline=\"\", encoding=\"utf_8_sig\") as fp:  #utf_8_sig:能讓輸出的csv正確顯示中文(utf_8會有亂碼)\n",
    "        writer = csv.writer(fp)\n",
    "        for item in items:\n",
    "            writer.writerow(item)\n",
    "    \n",
    "# 開始爬蟲\n",
    "if __name__ == \"__main__\":\n",
    "    #在電腦週邊搜尋\n",
    "    url = \"https://ph.xiapibuy.com/search?category=18596&keyword={0}&subcategory=18613&page={1}\"\n",
    "    \n",
    "    urls = get_urls(url, \"genius\", 0, 3)\n",
    "    \n",
    "    m = scraping(urls)\n",
    "    save_to_csv(m, \"m.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遍歷【越南蝦皮】各商品爬取細節_20200619完整版\n",
    "1. 先取得搜尋頁數的網址List\n",
    "2. 進入每個商品頁面\n",
    "3. 爬取進入頁面之商品細節"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://vn.xiapibuy.com/search?category=13030&keyword=genius&page11&sortBy=sales']\n",
      "Crawing No.1 Item in Total:50Item\n",
      "Crawing No.2 Item in Total:50Item\n",
      "Crawing No.3 Item in Total:50Item\n",
      "Crawing No.4 Item in Total:50Item\n",
      "Crawing No.5 Item in Total:50Item\n",
      "Crawing No.6 Item in Total:50Item\n",
      "Crawing No.7 Item in Total:50Item\n",
      "Crawing No.8 Item in Total:50Item\n",
      "Crawing No.9 Item in Total:50Item\n",
      "Crawing No.10 Item in Total:50Item\n",
      "Crawing No.11 Item in Total:50Item\n",
      "Crawing No.12 Item in Total:50Item\n",
      "Crawing No.13 Item in Total:50Item\n",
      "Crawing No.14 Item in Total:50Item\n",
      "Crawing No.15 Item in Total:50Item\n",
      "Crawing No.16 Item in Total:50Item\n",
      "Crawing No.17 Item in Total:50Item\n",
      "Crawing No.18 Item in Total:50Item\n",
      "Crawing No.19 Item in Total:50Item\n",
      "Crawing No.20 Item in Total:50Item\n",
      "Crawing No.21 Item in Total:50Item\n",
      "Crawing No.22 Item in Total:50Item\n",
      "Crawing No.23 Item in Total:50Item\n",
      "Crawing No.24 Item in Total:50Item\n",
      "Crawing No.25 Item in Total:50Item\n",
      "Crawing No.26 Item in Total:50Item\n",
      "Crawing No.27 Item in Total:50Item\n",
      "Crawing No.28 Item in Total:50Item\n",
      "Crawing No.29 Item in Total:50Item\n",
      "Crawing No.30 Item in Total:50Item\n",
      "Crawing No.31 Item in Total:50Item\n",
      "Crawing No.32 Item in Total:50Item\n",
      "Crawing No.33 Item in Total:50Item\n",
      "Crawing No.34 Item in Total:50Item\n",
      "Crawing No.35 Item in Total:50Item\n",
      "Crawing No.36 Item in Total:50Item\n",
      "Crawing No.37 Item in Total:50Item\n",
      "Crawing No.38 Item in Total:50Item\n",
      "Crawing No.39 Item in Total:50Item\n",
      "Crawing No.40 Item in Total:50Item\n",
      "Crawing No.41 Item in Total:50Item\n",
      "Crawing No.42 Item in Total:50Item\n",
      "Crawing No.43 Item in Total:50Item\n",
      "Crawing No.44 Item in Total:50Item\n",
      "Crawing No.45 Item in Total:50Item\n",
      "Crawing No.46 Item in Total:50Item\n",
      "Crawing No.47 Item in Total:50Item\n",
      "Crawing No.48 Item in Total:50Item\n",
      "Crawing No.49 Item in Total:50Item\n",
      "Crawing No.50 Item in Total:50Item\n"
     ]
    }
   ],
   "source": [
    "import re, time, requests, csv\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "#解析(蝦皮headers要用Googlebot)\n",
    "def get_soup(url):\n",
    "    headers = {\"user-agent\": \"Googlebot\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    return BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "#搜尋網址&換頁\n",
    "def get_urls(url, query, start_page, end_page): \n",
    "    urls = []\n",
    "    \n",
    "    for page in range(start_page, end_page+1):\n",
    "        urls.append(url.format(query, page))    #query帶入url的{0}、page帶入{1}\n",
    "    print(urls)    \n",
    "    return urls\n",
    "\n",
    "# 依序爬取每頁點入網址\n",
    "def FindLinks(pages):\n",
    "    linklist = []\n",
    "    for page in pages:  \n",
    "        soup = get_soup(page)\n",
    "        links = soup.find_all(\"div\",class_=\"col-xs-2-4 shopee-search-item-result__item\")\n",
    "        for link in links:\n",
    "            k = \"https://vn.xiapibuy.com/\" + link.find(\"a\").get('href')\n",
    "            linklist.append(k)\n",
    "    return linklist\n",
    "\n",
    "# 爬取點入分頁資料\n",
    "def get_goods(url):\n",
    "    goods = []\n",
    "    rows = get_soup(url)\n",
    "\n",
    "    for row in rows:\n",
    "\n",
    "        try:\n",
    "            name = row.find('div', class_=\"qaNIZv\").text\n",
    "        except:\n",
    "            name = None\n",
    "\n",
    "        try:\n",
    "            #price = row.find('div', class_=\"_3n5NQx\").text.replace(\"Rp\",\"\")\n",
    "            price = row.select_one(\"._3n5NQx\").get_text().replace(\"Rp\",\"\")\n",
    "        except:\n",
    "            price = None\n",
    "            \n",
    "        try:\n",
    "            Original_price = row.find(\"div\", class_=\"_3_ISdg\").text.replace(\"Rp\",\"\")\n",
    "        except:\n",
    "            Original_price = None\n",
    "            \n",
    "        try:\n",
    "            star = row.find('div', '_3Oj5_n _2z6cUg').text\n",
    "        except:\n",
    "            star = None\n",
    "\n",
    "        try:\n",
    "            #reviews = row.select(\"._3Oj5_n\")[1].get_text()\n",
    "            reviews = row.find_all('div', '_3Oj5_n')[1].text\n",
    "        except:\n",
    "            reviews = None\n",
    "            \n",
    "        try:\n",
    "            sold = row.find('div', '_22sp0A').text\n",
    "        except:\n",
    "            sold = None\n",
    "            \n",
    "        try:\n",
    "            #stock = row.select(\"._1FzU2Y .items-center div+ div\")[0].get_text()\n",
    "            stock = row.find(\"div\", \"flex items-center _1FzU2Y\").get_text()\n",
    "        except:\n",
    "            stock = None\n",
    "        \n",
    "        try:\n",
    "            seller = row.select_one(\"._3Lybjn\").get_text()\n",
    "        except:\n",
    "            seller = None\n",
    "            \n",
    "        try:\n",
    "            seller_link = \"https://vn.xiapibuy.com\" + row.find(\"a\", \"btn btn-light btn--s btn--inline btn-light--link Ed2lAD\").get('href')\n",
    "        except:\n",
    "            seller_link = None\n",
    "            \n",
    "        try:\n",
    "            seller_from = row.select(\".kIo6pj\")[-1].div.get_text()\n",
    "            #seller_from = row.find_all(\"kIo6pj\")[-1].text\n",
    "        except:\n",
    "            seller_from = None\n",
    "            \n",
    "        try:\n",
    "            category = row.select_one(\".kIo6pj\").get_text().replace(\"Kategori\", \"\")\n",
    "        except:\n",
    "            category = None\n",
    "\n",
    "        try:\n",
    "            brand = row.select_one(\"._2H-513\").get_text()\n",
    "        except:\n",
    "            brand = None\n",
    "\n",
    "        try:\n",
    "            description = row.find(\"div\", \"_2u0jt9\").get_text()\n",
    "        except:\n",
    "            description = None\n",
    "            \n",
    "        try:\n",
    "            URL = url\n",
    "        except:\n",
    "            URL = None\n",
    "            \n",
    "        good= [name, price, Original_price, star, reviews, sold, stock, seller, seller_link, seller_from, category, brand, description, URL]\n",
    "        goods.append(good)\n",
    "        \n",
    "    return goods[1]  #因為不知為何第[0]列都會出現一排None，只好取第[1]列\n",
    "\n",
    "# 將每一個點入頁面的List依序爬取\n",
    "def scraping(urls):\n",
    "    all_goods = [[\"name\",\"price\",\"Original_price\",\"star\",\"reviews\",\"sold\",\"stock\",\"seller\",\"seller_link\",\"seller_from\",\"category\",\"brand\",\"description\", \"URL\"]]\n",
    "    for idx,i in enumerate(FindLinks(urls)):  #記錄目前進行的迴圈次數，配上總迴圈次數，可做為進度條使用。\n",
    "        print(\"Crawing No.\" + str(idx+1) + \" Item in Total:\" + str(len(FindLinks(urls))) + \"Item\")\n",
    "        \n",
    "        goods = get_goods(i)\n",
    "        time.sleep(0.2)\n",
    "        all_goods.append(goods)\n",
    "    return all_goods\n",
    "\n",
    "#存成CSV\n",
    "def save_to_csv(items, file):\n",
    "    with open(file, \"w+\", newline=\"\", encoding=\"utf_8_sig\") as fp:  #utf_8_sig:能讓輸出的csv正確顯示中文(utf_8會有亂碼)\n",
    "        writer = csv.writer(fp)\n",
    "        for item in items:\n",
    "            writer.writerow(item)\n",
    "    \n",
    "# 開始爬蟲\n",
    "if __name__ == \"__main__\":\n",
    "    #在電腦週邊搜尋+銷售中\n",
    "    url = \"https://vn.xiapibuy.com/search?category=13030&keyword={0}&page={1}&sortBy=sales\"\n",
    "    urls = get_urls(url, \"genius\", 11, 11)\n",
    "    \n",
    "    m = scraping(urls)\n",
    "    save_to_csv(m, \"m.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 撈取深網中的資料-蝦皮購物API\n",
    "https://freelancerlife.info/zh/blog/%E6%92%88%E5%8F%96%E6%B7%B1%E7%B6%B2%E4%B8%AD%E7%9A%84%E8%B3%87%E6%96%99-%E8%9D%A6%E7%9A%AE%E8%B3%BC%E7%89%A9api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "深層網路(Deep Web)是指無法被搜索引擎搜索到的網站內容。使用瀏覽器拜訪一個網站，除了一個主要的html檔之外，對方的伺服器同時會載入許多檔案讓這個網頁能正常運作，例如:圖片檔、css、javascript、json檔等，這些檔案也都會有各自的網址，但並不會被搜索引擎給包含到，也就屬於深層網路的部分。\n",
    "\n",
    "當你要進行爬蟲時，務必要先檢查這些深層網路的檔案內容，有時候你想要的網頁資料通通都藏在這裡，讓你可以很簡便的抓取。\n",
    "\n",
    "要如何查看這些檔案呢?很簡單，使用你的瀏覽器就可以了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "首先前往要檢查的網站，在任意地方按下右鍵，選擇檢查網站，此時你的瀏覽器視窗會被新冒出來的檢查視窗分割。\n",
    "\n",
    "點選網路，按下重新整理(或F5)，這時候就可以一一點選查看所右網頁讀取進來的檔案。\n",
    "\n",
    "延續上一篇教學，我依然使用蝦皮購物網進行示範，操作流程示範影片如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "要檢查什麼呢?\n",
    "\n",
    "首先查看html這裡，這個主網頁是哪種請求方法?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "這個網站的請求方法是GET。除了GET之外還會有POST的請求方法，這就必須使用不同的程式碼來進行爬蟲。\n",
    "而如果是POST的話，則需要再察看參數，在使用進行爬蟲時需要在額外輸入這些參數，這部分留到下一篇教學再仔細跟各位介紹。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://freelancerlife.info/media/original_images/check_parameter.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "這裡顯示參數的名稱為keyword，內容為switch主機，因為是以GET的方式請求，因此參數就直接在網址裡面。\n",
    "\n",
    "接著檢查XHR，有些網站會以額外的方式將資料以json或xml的格式送進網頁中，如果你能直接獲取這些檔案，那就不用去處理解析網站原始碼的部分。\n",
    "\n",
    "在XHR中有許多網址，逐一檢查，這裡很多網址都包含了api，很有可能就是用來傳輸資料給網頁。\n",
    "\n",
    "例如這裡的網址就非常的可疑， \n",
    "https://shopee.tw/api/v2/search_items/?by=relevancy&keyword=switch%E4%B8%BB%E6%A9%9F&limit=50&newest=0&order=desc&page_type=search\n",
    "\n",
    "，既提到api又寫到seach_items，很有可能資料都在這裡喔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "直接用瀏覽器前往這個網址，商品資料果然在這裡。\n",
    "但是很多資料都是缺值，例如我們最在乎的商品價格。所以無法以api的方式查詢商品價格嗎?\n",
    "\n",
    "我們在嘗試隨便前往任一商品的分頁檢查，看看有沒有類似的api來傳送商品價格。\n",
    "\n",
    "https://shopee.tw/Nintendo-Switch%E4%B8%BB%E6%A9%9F-%E9%9B%BB%E5%85%89%E7%B4%85%E8%97%8D(%E5%8F%B0%E7%81%A3%E5%85%AC%E5%8F%B8%E8%B2%A8)-i.39500389.2495200573"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "找到一個很可疑的網址，這裡面會不會有商品價格呢?\n",
    "\n",
    "https://shopee.tw/api/v2/item/get?itemid=2495200573&shopid=39500389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "有!\n",
    "\n",
    "看來我們可以利用這兩個api來進行爬蟲，抓取商品價格。\n",
    "\n",
    "邏輯大概是這樣子的，在第一個api中輸入關鍵字，將裡面的itemid和shopid抓取出來，放到第二個api底下去查詢商品價格。\n",
    "\n",
    "OK~我們可以開始寫程式以這條路徑爬取商品價格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用requests與json套件，json套件可以將json格式轉成python的dict，讓我們可以很簡單的讀取到內容。\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "嘗試獲取第一個api的資訊，商品名稱、itemid、shopid:\n",
    "\n",
    "一樣要使用User-Agent為Googlebot才可以喔，不然會被重新導向，抓不到任何資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-cc85a009f680>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mapi1_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# 查看第8筆資料的內容:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "url = 'https://shopee.tw/api/v2/search_items/?by=relevancy&keyword=switch&limit=50'\n",
    "\n",
    "headers = {\n",
    "        'User-Agent': 'Googlebot',\n",
    "    }\n",
    "\n",
    "r = requests.get(url,headers=headers)\n",
    "\n",
    "api1_data = json.loads(r.text)\n",
    "\n",
    "# 查看第8筆資料的內容:\n",
    "print(api1_data['items'][8]['name'])\n",
    "print(api1_data['items'][8]['itemid'])\n",
    "print(api1_data['items'][8]['shopid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "好，接著使用這組itemid和shopid來爬取這筆商品的價格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemid = api1_data['items'][8]['itemid']\n",
    "shopid = api1_data['items'][8]['shopid']\n",
    "url2 = f'https://shopee.tw/api/v2/item/get?itemid={itemid}&shopid={shopid}'\n",
    "\n",
    "r = requests.get(url2,headers=headers)\n",
    "api2_data = json.loads(r.text)\n",
    "print(api2_data['item']['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "多出了五個零，沒關係，移掉就好。\n",
    "\n",
    "OK~\n",
    "\n",
    "我們可以很簡單的編寫爬蟲程式獲取商品價格，最後在把上面的程式改寫成好用函式吧~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def shopeeAPI_Scraper(keyword, n_items):\n",
    "\n",
    "    url1 = f'https://shopee.tw/api/v2/search_items/?by=relevancy&keyword={keyword}&limit={n_items}'\n",
    "    headers = {'User-Agent': 'Googlebot',}\n",
    "    r = requests.get(url1,headers=headers)\n",
    "    api1_data = json.loads(r.text)\n",
    "    \n",
    "    for i in range(n_items):\n",
    "        itemid = api1_data['items'][i]['itemid']\n",
    "        shopid = api1_data['items'][i]['shopid']\n",
    "        \n",
    "        url2 = f'https://shopee.tw/api/v2/item/get?itemid={itemid}&shopid={shopid}'\n",
    "        r = requests.get(url2,headers=headers)\n",
    "        api2_data = json.loads(r.text)\n",
    "        output = api1_data['items'][i]['name'].ljust(50) +': ' + str(api2_data['item']['price']/100000)\n",
    "        print(output)\n",
    "        time.sleep(0.2)\n",
    "    \n",
    "    \n",
    "    \n",
    "shopeeAPI_Scraper(keyword = '溜冰鞋', n_items = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
