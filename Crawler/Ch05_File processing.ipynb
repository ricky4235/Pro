{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.itread01.com/content/1544469144.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 開啟檔案 open()<br>\n",
    "open('檔案路徑/檔名', mode='模式')\n",
    "\n",
    "## 基本模式(mode='?')：\n",
    "### 1. \"r\" 唯讀模式:\n",
    "* 從指定的檔案讀取資料時，並不能夠對這個檔案的內容進行更動。\n",
    "* 如果我們所指定的檔案不存在，將會產生 FileNotFoundError 的例外。\n",
    "\n",
    "### 2. \"w\" 寫入模式 - 覆寫 :\n",
    "* 意即要來進行檔案的寫入，\n",
    "* 會在開啟的位置直接覆蓋掉原本的檔案。\n",
    "* 如果我們所指定的檔案路徑/名稱不存在，會新增一個新的檔案。\n",
    "\n",
    "### 3. \"a\" 寫入模式 - 續寫 :\n",
    "* 一樣是寫入模式\n",
    "* 會從原本的檔案\"最後\"繼續進行寫入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整模式\n",
    "r - 讀取(檔案需存在)\n",
    "\n",
    "w - 新建檔案寫入(檔案可不存在，若存在則清空)\n",
    "\n",
    "a - 資料附加到舊檔案後面(游標指在檔案結尾(EOF-End Of File))\n",
    "\n",
    "r+ - 讀取舊資料並寫入(檔案需存在且游標指在開頭)\n",
    "\n",
    "w+ - 清空檔案內容，新寫入的東西可在讀出(檔案可不存在，會自行新增)\n",
    "\n",
    "a+ - 資料附加到舊檔案後面(游標指在EOF)，可讀取資料\n",
    "\n",
    "b - 二進位模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data1,Data2,Data3\n",
      "10,33,45\n",
      "5,25,56\n"
     ]
    }
   ],
   "source": [
    "#Ch5_4_1\n",
    "import csv\n",
    "\n",
    "csvfile = \"Example.csv\"\n",
    "with open(csvfile, 'r') as fp:\n",
    "    reader = csv.reader(fp)\n",
    "    for row in reader:\n",
    "        print(','.join(row))  #print的更動是在python讀出檔案後做的，所以不影響原檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ch5_4_1a\n",
    "import csv\n",
    "\n",
    "csvfile = \"Example2.csv\"\n",
    "list1 = [[10,33,45], [5, 25, 567]]\n",
    "with open(csvfile, 'w+', newline='') as fp:  \n",
    "#這裡在開啟csv檔案時加上了newline=''參數，這是為了讓資料中包含的換行字元可以正確被解析，所以建議在讀取 csv 檔案時都固定加入這個參數。\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow([\"Data1\",\"Data2\",\"Data3\"])\n",
    "    for row in list1:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ch5_4_1b\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = \"https://www.w3schools.com/html/html_media.asp\"\n",
    "csvfile = \"VideoFormat.csv\"\n",
    "r = requests.get(url)\n",
    "r.encoding = \"utf8\"\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "tag_table = soup.find(class_=\"w3-table-all\")  # 找到<table>\n",
    "rows = tag_table.findAll(\"tr\")   # 找出所有<tr>\n",
    "# 開啟CSV檔案寫入截取的資料\n",
    "with open(csvfile, 'w+', newline='', encoding=\"utf-8\") as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    for row in rows:\n",
    "        rowList = []\n",
    "        for cell in row.findAll([\"td\", \"th\"]):\n",
    "            rowList.append(cell.get_text().replace(\"\\n\", \"\").replace(\"\\r\", \"\"))  #\\r是回車:回到行首\n",
    "        writer.writerow(rowList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Joe Chen\", \"score\": 95, \"tel\": \"0933123456\"}\n",
      "{'name': 'Joe Chen', 'score': 95, 'tel': '0933123456'}\n"
     ]
    }
   ],
   "source": [
    "#Ch5_4_2\n",
    "import json\n",
    "\n",
    "data = {\n",
    "   \"name\": \"Joe Chen\", \n",
    "   \"score\": 95, \n",
    "   \"tel\": \"0933123456\"         \n",
    "}\n",
    "\n",
    "json_str = json.dumps(data)\n",
    "print(json_str)\n",
    "data2 = json.loads(json_str)\n",
    "print(data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ch5_4_2a\n",
    "import json\n",
    "\n",
    "data = {\n",
    "   \"name\": \"Joe Chen\", \n",
    "   \"score\": 95, \n",
    "   \"tel\": \"0933123456\"        \n",
    "}\n",
    "\n",
    "jsonfile = \"Example.json\"\n",
    "with open(jsonfile, 'w') as fp:\n",
    "    json.dump(data, fp)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"Joe Chen\", \"score\": 95, \"tel\": \"0933123456\"}\n"
     ]
    }
   ],
   "source": [
    "#Ch5_4_2b\n",
    "import json\n",
    "\n",
    "jsonfile = \"Example.json\"\n",
    "with open(jsonfile, 'r') as fp:\n",
    "    data = json.load(fp)\n",
    "json_str = json.dumps(data)    \n",
    "print(json_str)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ch5_4_2c\n",
    "import json\n",
    "import requests\n",
    "\n",
    "url = \"https://www.googleapis.com/books/v1/volumes?maxResults=5&q=Python&projection=lite\"\n",
    "jsonfile = \"Books.json\"\n",
    "r = requests.get(url)\n",
    "r.encoding = \"utf8\"\n",
    "json_data = json.loads(r.text)\n",
    "with open(jsonfile, 'w') as fp:\n",
    "    json.dump(json_data, fp)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "錯誤! HTTP請求失敗...\n"
     ]
    }
   ],
   "source": [
    "#Ch5_5\n",
    "import requests\n",
    "\n",
    "url = \"http://hueyanchen.myweb.hinet.net/fchart05.png\"\n",
    "path = \"fchart05.png\"\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(path, 'wb') as fp:\n",
    "        for chunk in response:\n",
    "            fp.write(chunk)\n",
    "    print(\"圖檔已經下載\")        \n",
    "else:\n",
    "    print(\"錯誤! HTTP請求失敗...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 503: Service Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-af4455cab88f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"http://hueyanchen.myweb.hinet.net/fchart05.png\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fchart06.png\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 641\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 503: Service Unavailable"
     ]
    }
   ],
   "source": [
    "#Ch5_5a\n",
    "import urllib.request\n",
    "\n",
    "url = \"http://hueyanchen.myweb.hinet.net/fchart05.png\"\n",
    "response = urllib.request.urlopen(url)\n",
    "fp = open(\"fchart06.png\", \"wb\")\n",
    "size = 0\n",
    "while True:\n",
    "    info = response.read(10000)\n",
    "    if len(info) < 1:\n",
    "        break\n",
    "    size = size + len(info)\n",
    "    fp.write(info)    \n",
    "print(size, \"個字元下載...\")\n",
    "fp.close()\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png\n",
      "圖檔logo.png已經下載\n"
     ]
    }
   ],
   "source": [
    "#Ch5_5b\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://www.google.com.tw\"\n",
    "path = \"logo.png\"\n",
    "r = requests.get(url)\n",
    "r.encoding = \"utf8\"\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "tag_a = soup.find(id=\"hplogo\")\n",
    "# 取出Logo圖片的正規運算式\n",
    "match = re.search(r\"(/[^/#?]+)+\\.(?:jpg|gif|png)\", str(tag_a))\n",
    "print(match.group())\n",
    "url = url + str(match.group())\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(path, 'wb') as fp:\n",
    "        for chunk in response:\n",
    "            fp.write(chunk)\n",
    "    print(\"圖檔logo.png已經下載\")        \n",
    "else:\n",
    "    print(\"錯誤! HTTP請求失敗...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img alt=\"Google\" height=\"92\" id=\"hplogo\" src=\"/images/branding/googlelogo/1x/googlelogo_white_background_color_272x92dp.png\" style=\"padding:28px 0 14px\" width=\"272\"/>\n"
     ]
    }
   ],
   "source": [
    "#Ch5_5c\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://www.google.com.tw\"\n",
    "r = requests.get(url)\n",
    "r.encoding = \"big5\"\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "tag_a = soup.find(id=\"hplogo\")\n",
    "print(tag_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 關閉檔案 close()\n",
    "當我們在程式當中開啟了檔案以後，如果要停止對於這個檔案的更動或寫入，可以將檔案關閉。此時所使用的函式如下：<br>\n",
    "f.close()<br>\n",
    "當我們要開啟其他檔案時，當然可以直接再開啟然後取代掉目前的 file object，但是記得在使用時關閉檔案依然是一個好習慣。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取檔案 read()\n",
    "file.read([size])<br>\n",
    "當我們設置了 size 的值，電腦就會自動讀到指定的字節數量，若沒有設置就會將整個檔案都讀取進來。<br>\n",
    "ex:file.read(6)就只讀取前六個字節。<br>\n",
    "    \n",
    "file.readline()<br>\n",
    "讀取檔案中的整行資料，但是一次只讀取一行，包含 \\n 字元，如果檔案當中包含了 N 行的資料，我們就必須呼叫 f.readline() N 次。<br>\n",
    "\n",
    "file.readlines()<br>\n",
    "將檔案當中的所有資料都逐行讀取進來，然後會將其回傳成為一個 list。<br>\n",
    "由此可見， f.readlines() 會將檔案當中每一行當作是一個字串，然後存進串列當中回傳。因此 for loop經常與 f.readlines() 搭配使用，方法如下：<br>\n",
    "for line in f.readlines():<br>\n",
    "    print(line)<br>\n",
    "\n",
    "除此之外， Python 還給了我們一個幾乎一模一樣的用法，在不使用 f.readlines() 的情況下，<br>\n",
    "我們可以直接利用 for loop 對於我們的 file object f 來 iterate，方法如下：<br>\n",
    "for line in f:<br>\n",
    "    print(line)<br>\n",
    "    \n",
    "我們會得到相同的執行結果<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 寫入檔案 write()\n",
    "\n",
    "file.write('string')\n",
    "參數的資料型態是字串\n",
    "\n",
    "file.writelines(seq)\n",
    "seq參數必須是一個序列，也就是 list 或是 tuple 這類的資料型態。<br>\n",
    "\n",
    "print()\n",
    "當然，貼心的 Python 也提供了我們利用熟悉的 print() 就可以完成輸出到檔案的方法。<br>\n",
    "當我們今天想要將任何東西輸出時，都會直接使用 print() ，然後頂多更改參數 sep 或是 end 。<br>\n",
    "然而，除了這兩個參數以外，還有第三個參數 file 。方法如下：<br>\n",
    "f = open('movies.txt','w')\n",
    "print(list, file = f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在檔案中移動位置\n",
    "\n",
    "接著，我們要來介紹如何更進一步的進行檔案的處理。當我們在進行檔案的讀取時，有時可能會想要將檔案從頭到尾讀取第二遍。<br>\n",
    "那麼，當我們想要將上面的檔案內容印出兩次時，可能會寫出如下的程式碼進行以下的操作：<br>\n",
    "f = open('sample.txt')<br>\n",
    "for line in f:<br>\n",
    "    print(line)<br>\n",
    "for line in f:<br>\n",
    "    print(line)<br>\n",
    "f.close()<br>\n",
    "在我們的想像當中，上面的程式碼在第一個 for loop 跑完以後，下一個 for loop 應該要將檔案的內容在輸出一次，<br>\n",
    "也就是說，我們心裡想的輸出結果應該要是這樣子的：<br>\n",
    "123<br>\n",
    "456<br>\n",
    "789<br>\n",
    "123<br>\n",
    "456<br>\n",
    "789<br>\n",
    "然而，執行結果卻如下：<br>\n",
    "123<br>\n",
    "456<br>\n",
    "789<br>\n",
    "很顯然的，第二次的 for loop 完全沒有印出東西來。<br>\n",
    "原因是這樣子的，當我們在讀取或是寫入檔案時，可以想像成電腦裡面有一個指標，<br>\n",
    "類似於我們在使用 word 或是任何的文字編輯器時的指標，我們會依照他的位置，<br>\n",
    "知道我們接下來如果要打字會出現在什麼地方。<br>\n",
    "同樣的，當 file object 在處理檔案時也是一樣的概念。在上面的範例當中，<br>\n",
    "第一個 for loop 結束以後，這個指針隨著我們的讀取已經到了文件的最後面。<br>\n",
    "因此，接下來想要再使用 for loop 來讀取的話，如果我們沒有將指針移到檔案最前面，就無法得到任何東西。<br>\n",
    "因此，以下要介紹用來移動這個指針或是獲取現在指針位置的方法。<br>\n",
    "\n",
    "# file.seek()\n",
    "第一個是 f.seek() ，顧名思義，就是在檔案當中尋找。不過這裡說的尋找指的並不是尋找某個字或是某個資料，而是尋找某個位置，用法如下。<br>\n",
    "f = open('sample.txt')<br>\n",
    "for line in f:<br>\n",
    "    print(line)<br>\n",
    "f.seek(0)<br>\n",
    "for line in f:<br>\n",
    "    print(line)<br>\n",
    "f.close()<br>\n",
    "結果就會是兩遍：<br>\n",
    "123<br>\n",
    "456<br>\n",
    "789<br>\n",
    "123<br>\n",
    "456<br>\n",
    "789<br>\n",
    "\n",
    "# file.tell()\n",
    "他和 f.seek() 接近於相對的概念，所謂的 f.tell() ，顧名思義就是會回傳給你現在指標所在的位置。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 儲存成CSV檔案\n",
    "這隻爬蟲會去ezprice上根據指定的商品字眼搜集商品資訊, 並且將爬到的資訊儲存至csv檔案裡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import csv\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "EZPRICE_URL = 'https://ezprice.com.tw'\n",
    "CSV_FILE_NAME = 'ezprice.csv'\n",
    "\n",
    "\n",
    "def get_web_content(url):\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        print('Invalid url: ' + resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        return resp.text\n",
    "\n",
    "\n",
    "def get_price_info(query, page):\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    doms = list()\n",
    "    for page in range(1, page + 1):\n",
    "        url = EZPRICE_URL + '/s/%s/price/?q=%s&p=%s' % (encoded_query, encoded_query, str(page))\n",
    "        result_page = get_web_content(url)\n",
    "        doms.append(BeautifulSoup(result_page, 'html5lib'))\n",
    "    return doms\n",
    "\n",
    "\n",
    "def extract_results(dom):\n",
    "    items = list()\n",
    "    for div in dom.find_all('div', 'search-rst clearfix'):\n",
    "        item = list()\n",
    "        item.append(div.h4.a['title'])\n",
    "        item.append(div.find(itemprop='price')['content'])\n",
    "        if div.find('span', 'platform-name'):\n",
    "            item.append(div.find('span', 'platform-name').text.strip())\n",
    "        else:\n",
    "            item.append('N/A')\n",
    "        items.append(item)\n",
    "    return items, len(items)\n",
    "\n",
    "\n",
    "def show_results(items):\n",
    "    for item in items:\n",
    "        print(item)\n",
    "\n",
    "\n",
    "def write_to_csv_file(is_first_page, items):\n",
    "    with open(CSV_FILE_NAME, 'a', encoding='UTF-8', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "\n",
    "\n",
    "def read_from_csv_file():\n",
    "    print('\\nRead from csv file: ' + CSV_FILE_NAME)\n",
    "    with open(CSV_FILE_NAME, 'r', encoding='UTF-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            print(row['Item'], row['Price'], row['Store'])\n",
    "\n",
    "\n",
    "def main():\n",
    "    query = '吉胖喵'\n",
    "    page = 5\n",
    "    doms = get_price_info(query, page)\n",
    "    is_first_page = True\n",
    "    total_item_count = 0\n",
    "    for dom in doms:\n",
    "        items, count = extract_results(dom)\n",
    "        total_item_count += count\n",
    "        show_results(items)\n",
    "        write_to_csv_file(is_first_page, items)\n",
    "        is_first_page = False\n",
    "    print('There are %s items in %d page(s).' % (total_item_count, page))\n",
    "    read_from_csv_file()\n",
    "    # Uncomment this if you don't want to keep the data in csv file.\n",
    "    # os.remove(CSV_FILE_NAME)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
